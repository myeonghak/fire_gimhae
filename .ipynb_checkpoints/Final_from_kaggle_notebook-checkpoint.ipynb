{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import warnings\n",
    "import itertools as IT\n",
    "import glob\n",
    "import datetime as dt\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# from xgboost import plot_importance\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV   #Perforing grid search\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "# #Common Model Algorithms\n",
    "# from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "# import xgboost\n",
    "\n",
    "#Common Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "#Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.style.use('ggplot')\n",
    "sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 12,8\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows',100)\n",
    "pd.set_option('display.max_columns',500)\n",
    "pd.set_option('display.width',500)\n",
    "pd.set_option('max_info_columns',500)\n",
    "pd.set_option('display.float_format', lambda x : '%.3f' % x )\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import collections\n",
    "# Other Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "#from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "#from imblearn.under_sampling import NearMiss\n",
    "#from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import collections\n",
    "# Other Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "# Our data is already scaled we should split our training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "# Use GridSearchCV to find the best parameters.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "test= pd.read_csv(\"../input/fire-gimhae/PJT002_test.csv\")\n",
    "train = pd.read_csv(\"../input/fire-gimhae/PJT002_train.csv\")\n",
    "val = pd.read_csv(\"../input/fire-gimhae/PJT002_validation.csv\")\n",
    "\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)\n",
    "\n",
    "\n",
    "train['target_value']=train.fr_yn.apply(lambda x: 1 if x==\"Y\" else 0)\n",
    "val['target_value']=val.fr_yn.apply(lambda x: 1 if x==\"Y\" else 0)\n",
    "\n",
    "\n",
    "train.describe()\n",
    "test.describe()\n",
    "\n",
    "def crosstab_ratio(data,x):\n",
    "    if data[x].dtype == 'object' :\n",
    "        print('Fire Crosstab by : x')\n",
    "        print(pd.crosstab(data[x],data['fr_yn']).apply(lambda r: r/r.sum(), axis=1))\n",
    "        print('-'*10, '\\n')\n",
    "        \n",
    "def crosstab(data,x):\n",
    "    if data[x].dtype == 'object' :\n",
    "        print('Fire Crosstab by : x')\n",
    "        print(pd.crosstab(data[x],data['fr_yn']))\n",
    "        print('-'*10, '\\n')\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# 전처리\n",
    "\n",
    "## 건물\n",
    "\n",
    "object_list=[]\n",
    "\n",
    "for col in train:\n",
    "    if train[col].dtype ==\"object\":\n",
    "        object_list.append(col)\n",
    "                \n",
    "\n",
    "        \n",
    "object_train=train[object_list[:]]\n",
    "\n",
    "object_train.head(5)\n",
    "\n",
    "\n",
    "\n",
    "object_list=[]\n",
    "\n",
    "for col in val:\n",
    "    if val[col].dtype ==\"object\":\n",
    "        object_list.append(col)\n",
    "        \n",
    "object_val=val[object_list[:]]\n",
    "\n",
    "object_list=[]\n",
    "\n",
    "for col in test:\n",
    "    if test[col].dtype ==\"object\":\n",
    "        object_list.append(col)\n",
    "        \n",
    "object_test=test[object_list[:]]\n",
    "\n",
    "object_test.head(5)\n",
    "                \n",
    "\n",
    "print(len(object_val.columns))\n",
    "print(len(object_train.columns))\n",
    "print(set(object_train)-set(object_val))\n",
    "print(set(object_train)-set(object_test))\n",
    "\n",
    "# 이거 빼고 다 일치\n",
    "\n",
    "> ### bldng_us(건물용도)\n",
    "\n",
    "# train엔 있고 val엔 없는 데이터 출력 \n",
    "# vice versa\n",
    "\n",
    "print(set(object_train[\"bldng_us\"].unique())-set(object_val[\"bldng_us\"].unique()))\n",
    "print(set(object_val[\"bldng_us\"].unique())-set(object_train[\"bldng_us\"].unique()))\n",
    "\n",
    "print(set(object_train[\"bldng_us\"].unique())-set(object_test[\"bldng_us\"].unique()))\n",
    "print(set(object_test[\"bldng_us\"].unique())-set(object_train[\"bldng_us\"].unique()))\n",
    "\n",
    "only_train_bldng_us=list(set(object_train[\"bldng_us\"].unique())-set(object_test[\"bldng_us\"].unique()))\n",
    "\n",
    "# 결측치 탐색\n",
    "\n",
    "print(object_train[object_train[\"bldng_us\"].isna()].shape)\n",
    "\n",
    "print(object_val[object_val[\"bldng_us\"].isna()].shape)\n",
    "\n",
    "print(object_test[object_test[\"bldng_us\"].isna()].shape)\n",
    "\n",
    "object_train.groupby(\"bldng_us\").size().sort_values(ascending=False)\n",
    "\n",
    "object_val.groupby(\"bldng_us\").size().sort_values(ascending=False)\n",
    "\n",
    "object_test.groupby(\"bldng_us\").size().sort_values(ascending=False)\n",
    "\n",
    "crosstab(train,\"bldng_us\")\n",
    "\n",
    "crosstab(val,\"bldng_us\")\n",
    "\n",
    "\n",
    "object_train.drop(object_train[\"bldng_us\"].isin(only_train_bldng_us).index)\n",
    "\n",
    "# drop할지, None 할지 두고 보기\n",
    "\n",
    "object_train.drop(object_train[object_train[\"bldng_us\"].isin(only_train_bldng_us)].index,axis=0,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bldng_us_dummies = pd.get_dummies(object_train[\"bldng_us\"])\n",
    "object_train = object_train.join(bldng_us_dummies)\n",
    "\n",
    "bldng_us_dummies = pd.get_dummies(object_val[\"bldng_us\"])\n",
    "object_val = object_val.join(bldng_us_dummies)\n",
    "\n",
    "bldng_us_dummies = pd.get_dummies(object_test[\"bldng_us\"])\n",
    "object_test = object_test.join(bldng_us_dummies)\n",
    "\n",
    "\n",
    "object_train['bldng_archtctr'][object_train['bldng_archtctr'].isin(['기타콘크리트구조','통나무구조','프리케스트콘크리트구조'])]=None\n",
    "\n",
    "low = [\"목구조\", \"일반목구조\", \"벽돌구조\", \"블록구조\", \"석구조\", \"조적구조\"]\n",
    "normal = [\"강파이프구조\", \"경량철골구조\", \"기타강구조\", \"기타구조\", \"기타조적구조\"]\n",
    "high = [\"일반철골구조\", \"철골철근콘크리트구조\", \"철골콘크리트구조\", \"철근콘크리트구조\"]\n",
    "\n",
    "\n",
    "object_train['bldng_archt_danger']=object_train.bldng_archtctr.apply(lambda x: \"low\" if x in low else x)\n",
    "object_train['bldng_archt_danger']=object_train.bldng_archt_danger.apply(lambda x: \"normal\" if x in normal else x)\n",
    "object_train['bldng_archt_danger']=object_train.bldng_archt_danger.apply(lambda x: \"high\" if x in high else x)\n",
    "\n",
    "object_val['bldng_archt_danger']=object_val.bldng_archtctr.apply(lambda x: \"low\" if x in low else x)\n",
    "object_val['bldng_archt_danger']=object_val.bldng_archt_danger.apply(lambda x: \"normal\" if x in normal else x)\n",
    "object_val['bldng_archt_danger']=object_val.bldng_archt_danger.apply(lambda x: \"high\" if x in high else x)\n",
    "\n",
    "\n",
    "object_test['bldng_archt_danger']=object_test.bldng_archtctr.apply(lambda x: \"low\" if x in low else x)\n",
    "object_test['bldng_archt_danger']=object_test.bldng_archt_danger.apply(lambda x: \"normal\" if x in normal else x)\n",
    "object_test['bldng_archt_danger']=object_test.bldng_archt_danger.apply(lambda x: \"high\" if x in high else x)\n",
    "\n",
    "bldng_archt_dummies = pd.get_dummies(object_train[\"bldng_archt_danger\"])\n",
    "\n",
    "object_train = object_train.join(bldng_archt_dummies)\n",
    "\n",
    "bldng_archt_dummies = pd.get_dummies(object_val[\"bldng_archt_danger\"])\n",
    "object_val = object_val.join(bldng_archt_dummies)\n",
    "\n",
    "bldng_archt_dummies = pd.get_dummies(object_test[\"bldng_archt_danger\"])\n",
    "object_test = object_test.join(bldng_archt_dummies)\n",
    "\n",
    "### dt_of_athrztn 건물승인날짜\n",
    "\n",
    "object_val[\"dt_of_athrztn\"]=val['dt_of_athrztn']\n",
    "object_test[\"dt_of_athrztn\"]=test['dt_of_athrztn']\n",
    "\n",
    "object_train.dt_of_athrztn=object_train.dt_of_athrztn.apply(lambda x: str(x)[0:4] if len(str(x))==10 else x)\n",
    "object_train.dt_of_athrztn=object_train.dt_of_athrztn.apply(lambda x: str(x)[0:4] if len(str(x))==8 else x)\n",
    "object_train.dt_of_athrztn=object_train.dt_of_athrztn.apply(lambda x: str(x)[0:4] if len(str(x))==6 else x)\n",
    "\n",
    "object_val[\"dt_of_athrztn\"] = object_val[\"dt_of_athrztn\"].astype(str).str[:4]\n",
    "object_test[\"dt_of_athrztn\"]=object_test[\"dt_of_athrztn\"].astype(str).str[:4]\n",
    "\n",
    "object_train.dt_of_athrztn\n",
    "\n",
    "dirty_date_idx=object_train.dt_of_athrztn[object_train.dt_of_athrztn.astype(str).apply(len)>4].index\n",
    "\n",
    "object_train.dt_of_athrztn[dirty_date_idx[0]]=\"1980\"\n",
    "object_train.dt_of_athrztn[dirty_date_idx[1]]=\"1890\"\n",
    "object_train.dt_of_athrztn[dirty_date_idx[2]]=\"1890\"\n",
    "object_train.dt_of_athrztn[dirty_date_idx[3]]=\"1990\"\n",
    "object_train.dt_of_athrztn[dirty_date_idx[4]]=\"1990\"\n",
    "object_train.dt_of_athrztn[dirty_date_idx[5]]=\"1982\"\n",
    "object_train.dt_of_athrztn[dirty_date_idx[6]]=\"1978\"\n",
    "object_train.dt_of_athrztn[dirty_date_idx[7]]=\"1994\"\n",
    "\n",
    "dirty_date_idx2=object_train.dt_of_athrztn[(object_train.dt_of_athrztn.notnull())&(object_train.dt_of_athrztn.astype(str).apply(len)<4)].index\n",
    "\n",
    "#결측치가 아닌 연도 데이터를 시각화 하는 코드\n",
    "\n",
    "#sns.distplot(object_data.dt_of_athrztn[~(object_data.dt_of_athrztn.isna)()].astype(int))\n",
    "\n",
    "말도안돼_연도=object_train.dt_of_athrztn[~(object_train.dt_of_athrztn.isna)()].astype(int)[object_train.dt_of_athrztn[~(object_train.dt_of_athrztn.isna)()].astype(int)>2019].index\n",
    "\n",
    "object_train.dt_of_athrztn=object_train.dt_of_athrztn.apply(lambda x : \"1997\" if x==\"9712\" else x)\n",
    "\n",
    "object_train.dt_of_athrztn.loc[53193]\n",
    "\n",
    "있는_연도=object_train.dt_of_athrztn[~(object_train.dt_of_athrztn.isna())].astype(int)\n",
    "\n",
    "not_na_idx=object_train.dt_of_athrztn[object_train.dt_of_athrztn.notnull()].index\n",
    "\n",
    "object_train.dt_of_athrztn[not_na_idx]=object_train.dt_of_athrztn[not_na_idx].astype(int)\n",
    "\n",
    "object_train[\"Bldng_Age\"]=object_train.dt_of_athrztn.apply(lambda x: 2019-x)\n",
    "\n",
    "year_median=object_train[\"Bldng_Age\"][object_train[\"Bldng_Age\"].notnull()].median()\n",
    "\n",
    "object_train[\"Bldng_Age\"].fillna(year_median,inplace=True)\n",
    "\n",
    "object_val.loc[(object_val[\"dt_of_athrztn\"].isna()),\"dt_of_athrztn\"]= object_val[~(object_val[\"dt_of_athrztn\"].isna())][\"dt_of_athrztn\"].median()\n",
    "\n",
    "object_test.loc[(object_test[\"dt_of_athrztn\"].isna()),\"dt_of_athrztn\"]= object_test[~(object_test[\"dt_of_athrztn\"].isna())][\"dt_of_athrztn\"].median()\n",
    "\n",
    "object_val.dt_of_athrztn.replace(\"nan\",object_val[~(object_val[\"dt_of_athrztn\"].isna())][\"dt_of_athrztn\"].median(),inplace=True)\n",
    "object_test.dt_of_athrztn.replace(\"nan\",object_test[~(object_test[\"dt_of_athrztn\"].isna())][\"dt_of_athrztn\"].median(),inplace=True)\n",
    "\n",
    "\n",
    "object_val['Bldng_Age']=object_val.dt_of_athrztn.astype(int)\n",
    "object_test['Bldng_Age']=object_test.dt_of_athrztn.astype(int)\n",
    "\n",
    "object_val['Bldng_Age']=object_val['Bldng_Age'].apply(lambda x: 2019-x)\n",
    "object_test['Bldng_Age']=object_test['Bldng_Age'].apply(lambda x: 2019-x)\n",
    "\n",
    "criterion_age=20\n",
    "\n",
    "object_train.loc[object_train[\"Bldng_Age\"]<criterion_age,\"Bldng_old_YN\"] = \"old\"\n",
    "object_train.loc[object_train[\"Bldng_Age\"]>=criterion_age,\"Bldng_old_YN\"] = \"new\"\n",
    "\n",
    "object_val.loc[object_val[\"Bldng_Age\"]<criterion_age,\"Bldng_old_YN\"] = \"old\"\n",
    "object_val.loc[object_val[\"Bldng_Age\"]>=criterion_age,\"Bldng_old_YN\"] = \"new\"\n",
    "\n",
    "object_test.loc[object_test[\"Bldng_Age\"]<criterion_age,\"Bldng_old_YN\"] = \"old\"\n",
    "object_test.loc[object_test[\"Bldng_Age\"]>=criterion_age,\"Bldng_old_YN\"] = \"new\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rd_sd_nm\n",
    "\n",
    "# train엔 있고 val엔 없는 데이터 출력 \n",
    "\n",
    "# vice versa\n",
    "\n",
    "\n",
    "print(set(object_train[\"rd_sd_nm\"].unique())-set(object_val[\"rd_sd_nm\"].unique()))\n",
    "\n",
    "print(set(object_val[\"rd_sd_nm\"].unique())-set(object_train[\"rd_sd_nm\"].unique()))\n",
    "\n",
    "print(set(object_train[\"rd_sd_nm\"].unique())-set(object_test[\"rd_sd_nm\"].unique()))\n",
    "\n",
    "print(set(object_test[\"rd_sd_nm\"].unique())-set(object_train[\"rd_sd_nm\"].unique()))\n",
    "\n",
    "crosstab(object_train,\"rd_sd_nm\")\n",
    "crosstab_ratio(object_train,\"rd_sd_nm\")\n",
    "\n",
    "\n",
    "road_low = [\"광대로한면\",\"맹지\", \"세로각지(가)\", \"세로각지(불)\", \"세로한면(가)\", \"세로한면(불)\", \"소로한면\",\"지정되지않음\", \"소로각지\"]\n",
    "road_high = [ \"광대세각\", \"광대소각\",\"중로각지\",\"중로한면\"]\n",
    "\n",
    "\n",
    "object_train['road_danger']=object_train.rd_sd_nm.apply(lambda x: \"road_low\" if x in road_low else x)\n",
    "object_train['road_danger']=object_train.road_danger.apply(lambda x: \"road_high\" if x in road_high else x)\n",
    "\n",
    "object_val['road_danger']=object_val.rd_sd_nm.apply(lambda x: \"road_low\" if x in road_low else x)\n",
    "object_val['road_danger']=object_val.road_danger.apply(lambda x: \"road_high\" if x in road_high else x)\n",
    "\n",
    "object_test['road_danger']=object_test.rd_sd_nm.apply(lambda x: \"road_low\" if x in road_low else x)\n",
    "object_test['road_danger']=object_test.road_danger.apply(lambda x: \"road_high\" if x in road_high else x)\n",
    "\n",
    "### trgt_cnt 소방점검대상물기준\n",
    "\n",
    "object_train.loc[object_train[\"trgt_crtr\"]==\"자동화재탐지설치대상\", \"auto\"] = 1\n",
    "object_train.loc[object_train[\"trgt_crtr\"]!=\"자동화재탐지설치대상\", \"auto\"] = 0\n",
    "object_train.loc[object_train[\"trgt_crtr\"]==\"옥내소화전설치대상\", \"oknae\"] = 1\n",
    "object_train.loc[object_train[\"trgt_crtr\"]!=\"옥내소화전설치대상\", \"oknae\"] = 0\n",
    "object_train.loc[object_train[\"trgt_crtr\"]==\"스프링클러,물분무등설치대상\", \"sprinkler\"] = 1\n",
    "object_train.loc[object_train[\"trgt_crtr\"]!=\"스프링클러,물분무등설치대상\", \"sprinkler\"] = 0\n",
    "\n",
    "object_val.loc[object_val[\"trgt_crtr\"]==\"자동화재탐지설치대상\", \"auto\"] = 1\n",
    "object_val.loc[object_val[\"trgt_crtr\"]!=\"자동화재탐지설치대상\", \"auto\"] = 0\n",
    "object_val.loc[object_val[\"trgt_crtr\"]==\"옥내소화전설치대상\", \"oknae\"] = 1\n",
    "object_val.loc[object_val[\"trgt_crtr\"]!=\"옥내소화전설치대상\", \"oknae\"] = 0\n",
    "object_val.loc[object_val[\"trgt_crtr\"]==\"스프링클러,물분무등설치대상\", \"sprinkler\"] = 1\n",
    "object_val.loc[object_val[\"trgt_crtr\"]!=\"스프링클러,물분무등설치대상\", \"sprinkler\"] = 0\n",
    "\n",
    "object_test.loc[object_test[\"trgt_crtr\"]==\"자동화재탐지설치대상\", \"auto\"] = 1\n",
    "object_test.loc[object_test[\"trgt_crtr\"]!=\"자동화재탐지설치대상\", \"auto\"] = 0\n",
    "object_test.loc[object_test[\"trgt_crtr\"]==\"옥내소화전설치대상\", \"oknae\"] = 1\n",
    "object_test.loc[object_test[\"trgt_crtr\"]!=\"옥내소화전설치대상\", \"oknae\"] = 0\n",
    "object_test.loc[object_test[\"trgt_crtr\"]==\"스프링클러,물분무등설치대상\", \"sprinkler\"] = 1\n",
    "object_test.loc[object_test[\"trgt_crtr\"]!=\"스프링클러,물분무등설치대상\", \"sprinkler\"] = 0\n",
    "\n",
    "### us_yn 사용 여부\n",
    "\n",
    "us_yn = pd.get_dummies(object_train[\"us_yn\"])\n",
    "object_train = object_train.join(us_yn)\n",
    "\n",
    "us_yn = pd.get_dummies(object_val[\"us_yn\"])\n",
    "object_val = object_val.join(us_yn)\n",
    "\n",
    "us_yn = pd.get_dummies(object_test[\"us_yn\"])\n",
    "object_test = object_test.join(us_yn)\n",
    "\n",
    "\n",
    "### dngrs_thng_yn 위험물대상여부\n",
    "\n",
    "object_train.loc[object_train[\"dngrs_thng_yn\"].isnull(),\"dngrs_thng\"]= 0\n",
    "object_train.loc[object_train[\"dngrs_thng_yn\"].notnull(),\"dngrs_thng\"]= 1\n",
    "\n",
    "crosstab_ratio(object_train,\"dngrs_thng_yn\")\n",
    "\n",
    "object_val.loc[object_val[\"dngrs_thng_yn\"].isnull(),\"dngrs_thng\"]= 0\n",
    "object_val.loc[object_val[\"dngrs_thng_yn\"].notnull(),\"dngrs_thng\"]= 1\n",
    "\n",
    "object_test.loc[object_test[\"dngrs_thng_yn\"].isnull(),\"dngrs_thng\"]= 0\n",
    "object_test.loc[object_test[\"dngrs_thng_yn\"].notnull(),\"dngrs_thng\"]= 1\n",
    "\n",
    "\n",
    "object_train.drop([\"dt_of_fr\",\"bldng_us\",\"bldng_archtctr\",\"dt_of_athrztn\",\"bldng_us_clssfctn\",\"jmk\",\"rgnl_ar_nm\",\"rgnl_ar_nm2\",\"lnd_us_sttn_nm\",\"rd_sd_nm\",\"emd_nm\",\"mlt_us_yn\",\"trgt_crtr\",\"fr_fghtng_fclt_spcl_css_5_yn\",\"fr_fghtng_fclt_spcl_css_6_yn\"\\\n",
    "\n",
    ",\"us_yn\",\"dngrs_thng_yn\",\"slf_fr_brgd_yn\",\"blk_dngrs_thng_mnfctr_yn\",\"cltrl_hrtg_yn\",\"bldng_archt_danger\",\"Bldng_old_YN\",\"fr_yn\"],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "object_val.drop([\"dt_of_fr\",\"bldng_us\",\"bldng_archtctr\",\"dt_of_athrztn\",\"bldng_us_clssfctn\",\"jmk\",\"rgnl_ar_nm\",\"rgnl_ar_nm2\",\"lnd_us_sttn_nm\",\"rd_sd_nm\",\"emd_nm\",\"mlt_us_yn\",\"trgt_crtr\",\"fr_fghtng_fclt_spcl_css_5_yn\",\"fr_fghtng_fclt_spcl_css_6_yn\"\\\n",
    "\n",
    ",\"us_yn\",\"dngrs_thng_yn\",\"slf_fr_brgd_yn\",\"blk_dngrs_thng_mnfctr_yn\",\"cltrl_hrtg_yn\",\"bldng_archt_danger\",\"Bldng_old_YN\",\"fr_yn\",'교정및군사시설','기타콘크리트구조',\\\n",
    "                '묘지관련시설','방송통신시설','통나무구조'],axis=1,inplace=True)\n",
    "\n",
    "object_test.drop([\"dt_of_fr\",\"bldng_us\",\"bldng_archtctr\",\"dt_of_athrztn\",\"bldng_us_clssfctn\",\"jmk\",\"rgnl_ar_nm\",\"rgnl_ar_nm2\",\"lnd_us_sttn_nm\",\"rd_sd_nm\",\"emd_nm\",\"mlt_us_yn\",\"trgt_crtr\",\"fr_fghtng_fclt_spcl_css_5_yn\",\"fr_fghtng_fclt_spcl_css_6_yn\"\\\n",
    "\n",
    ",\"us_yn\",\"dngrs_thng_yn\",\"slf_fr_brgd_yn\",\"blk_dngrs_thng_mnfctr_yn\",\"cltrl_hrtg_yn\",\"bldng_archt_danger\",\"Bldng_old_YN\",'기타콘크리트구조',],axis=1,inplace=True)\n",
    "\n",
    "print(set(object_test.columns)-set(object_train.columns))\n",
    "print(set(object_test.columns)-set(object_val.columns))\n",
    "print(set(object_train.columns)-set(object_test.columns))\n",
    "print(set(object_train.columns)-set(object_val.columns))\n",
    "print(set(object_val.columns)-set(object_train.columns))\n",
    "print(set(object_val.columns)-set(object_test.columns))\n",
    "\n",
    "\n",
    "# 수치형 데이터 따로 정리\n",
    "\n",
    "numeric_list=list(set(train.columns.tolist()) - set(object_list))\n",
    "\n",
    "numeric_list=sorted(numeric_list)\n",
    "\n",
    "numeric_train=train[numeric_list[:]]\n",
    "\n",
    "\n",
    "numeric_list=list(set(val.columns.tolist()) - set(object_list))\n",
    "\n",
    "numeric_list=sorted(numeric_list)\n",
    "\n",
    "numeric_val=val[numeric_list[:]]\n",
    "\n",
    "\n",
    "\n",
    "numeric_list=list(set(test.columns.tolist()) - set(object_list))\n",
    "\n",
    "numeric_list=sorted(numeric_list)\n",
    "\n",
    "numeric_test=test[numeric_list[:]]\n",
    "\n",
    "numeric_train\n",
    "\n",
    "### bldng_cnt 건물채수\n",
    "\n",
    "small_crit=11\n",
    "middle_crit=74\n",
    "\n",
    "numeric_train.loc[numeric_train[\"bldng_cnt\"]<small_crit, \"bldng_cnt_size\"] = \"few\"\n",
    "numeric_train.loc[(numeric_train[\"bldng_cnt\"]>=small_crit)&(numeric_train[\"bldng_cnt\"]<middle_crit), \"bldng_cnt_size\"] = \"soso\"\n",
    "numeric_train.loc[numeric_train[\"bldng_cnt\"]>=middle_crit, \"bldng_cnt_size\"] = \"many\"\n",
    "\n",
    "small_crit=11\n",
    "middle_crit=74\n",
    "\n",
    "numeric_train.loc[train[\"bldng_cnt\"]<small_crit, \"bldng_cnt_encoded\"] = \"small\"\n",
    "numeric_train.loc[(train[\"bldng_cnt\"]>=small_crit)&(numeric_train[\"bldng_cnt\"]<middle_crit), \"bldng_cnt_encoded\"] = \"middle\"\n",
    "numeric_train.loc[train[\"bldng_cnt\"]>=middle_crit, \"bldng_cnt_encoded\"] = \"big\"\n",
    "\n",
    "crosstab_ratio(numeric_train,\"bldng_cnt_encoded\")\n",
    "\n",
    "one_hot_bldng_cnt = pd.get_dummies(numeric_train[\"bldng_cnt_size\"])\n",
    "numeric_train = numeric_train.join(one_hot_bldng_cnt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "numeric_val.loc[numeric_val[\"bldng_cnt\"]<small_crit, \"bldng_cnt_size\"] = \"few\"\n",
    "numeric_val.loc[(numeric_val[\"bldng_cnt\"]>=small_crit)&(numeric_val[\"bldng_cnt\"]<middle_crit), \"bldng_cnt_size\"] = \"soso\"\n",
    "numeric_val.loc[numeric_val[\"bldng_cnt\"]>=middle_crit, \"bldng_cnt_size\"] = \"many\"\n",
    "\n",
    "one_hot_bldng_cnt = pd.get_dummies(numeric_val[\"bldng_cnt_size\"])\n",
    "numeric_val = numeric_val.join(one_hot_bldng_cnt)\n",
    "\n",
    "\n",
    "\n",
    "numeric_test.loc[numeric_test[\"bldng_cnt\"]<small_crit, \"bldng_cnt_size\"] = \"few\"\n",
    "numeric_test.loc[(numeric_test[\"bldng_cnt\"]>=small_crit)&(numeric_test[\"bldng_cnt\"]<middle_crit), \"bldng_cnt_size\"] = \"soso\"\n",
    "numeric_test.loc[numeric_test[\"bldng_cnt\"]>=middle_crit, \"bldng_cnt_size\"] = \"many\"\n",
    "\n",
    "one_hot_bldng_cnt = pd.get_dummies(numeric_test[\"bldng_cnt_size\"])\n",
    "numeric_test = numeric_test.join(one_hot_bldng_cnt)\n",
    "\n",
    "### bldng_ar & ttl_ar & lnd_ar 건물건축면적, 건물연면적, 토지면적\n",
    "\n",
    "# val 값이 전처리 하기 전이 조금 더 잘나옴\n",
    "\n",
    "train[\"bldng_ar\"].head(10)\n",
    "\n",
    "\n",
    "\n",
    "bldng_ar_notnull = train[train[\"bldng_ar\"]!=0]\n",
    "sns.lmplot(data=bldng_ar_notnull, x=\"bldng_ar\", y=\"ttl_ar\", hue=\"fr_yn\", fit_reg=False)\n",
    "\n",
    "low_bldng_ar = train[(train[\"bldng_ar\"]<4000)&(train[\"bldng_ar\"]>0)&(train[\"ttl_ar\"]<3000)&(train[\"ttl_ar\"]>0)]\n",
    "low_bldng_ar.shape\n",
    "\n",
    "sns.lmplot(data=low_bldng_ar, x=\"bldng_ar\", y=\"ttl_ar\", hue=\"fr_yn\", fit_reg=False)\n",
    "\n",
    "low_bldng_ar = train[(train[\"bldng_ar\"]<4000)&(train[\"bldng_ar\"]>0)\n",
    "                     &(train[\"ttl_ar\"]<3000)&(train[\"ttl_ar\"]>0)\n",
    "                     &(train[\"lnd_ar\"]<4000)&(train[\"lnd_ar\"]>0)]\n",
    "\n",
    "sns.lmplot(data=low_bldng_ar, x=\"bldng_ar\", y=\"lnd_ar\", hue=\"fr_yn\", fit_reg=False, size=10)\n",
    "\n",
    "sns.lmplot(data=low_bldng_ar, x=\"ttl_ar\", y=\"lnd_ar\", hue=\"fr_yn\", fit_reg=False, size=10)\n",
    "\n",
    "low_low_bldng_ar = train[(train[\"bldng_ar\"]<1500)&(train[\"bldng_ar\"]>0)\n",
    "                     &(train[\"ttl_ar\"]<2000)&(train[\"ttl_ar\"]>0)\n",
    "                     &(train[\"lnd_ar\"]<2000)&(train[\"lnd_ar\"]>0)]\n",
    "low_low_bldng_ar.shape\n",
    "\n",
    "sns.lmplot(data=low_low_bldng_ar, x=\"bldng_ar\", y=\"ttl_ar\", hue=\"fr_yn\", fit_reg=False, size=10)\n",
    "\n",
    "train[train[\"bldng_ar\"]==0].shape\n",
    "\n",
    "train[\"bldng_ar\"].describe()\n",
    "\n",
    "train[\"ttl_ar\"].mean()\n",
    "\n",
    "train[\"ttl_ar\"].median()\n",
    "\n",
    "train.loc[train[\"bldng_ar\"]==0, \"bldng_ar\"] = train[\"bldng_ar\"].median()\n",
    "train.loc[train[\"ttl_ar\"]==0, \"ttl_ar\"] = train[\"ttl_ar\"].median()\n",
    "\n",
    "val.loc[val[\"bldng_ar\"]==0, \"bldng_ar\"] = val[\"bldng_ar\"].median()\n",
    "val.loc[val[\"ttl_ar\"]==0, \"ttl_ar\"] = val[\"ttl_ar\"].median()\n",
    "\n",
    "test.loc[test[\"bldng_ar\"]==0, \"bldng_ar\"] = test[\"bldng_ar\"].median()\n",
    "test.loc[test[\"ttl_ar\"]==0, \"ttl_ar\"] = test[\"ttl_ar\"].median()\n",
    "\n",
    "\n",
    "#train[\"floors\"] = train[\"ttl_ar\"] / train[\"bldng_ar\"]\n",
    "#train[\"floors\"].head()\n",
    "\n",
    "train = train.drop(\"lnd_ar\", 1)\n",
    "val = val.drop(\"lnd_ar\", 1)\n",
    "test = test.drop(\"lnd_ar\", 1)\n",
    "\n",
    "## part 3\n",
    "\n",
    "### ele_energy_us_YYYYMM 전기 에너지 사용량 (YYYY년 M월)\n",
    "\n",
    "### slf_fr_brgd_yn 자체소방대여부\n",
    "\n",
    "### blk_dngrs_thng_mnfctr_yn 대량위험물제조소등여부\n",
    "\n",
    "### cltrl_hrtg_yn 문화재여부\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train=numeric_train.merge(object_train,left_index=True,right_index=True)\n",
    "train=train.drop(['many','dt_of_athrztn','fr_yn',\"bldng_cnt_size\",\"road_danger\"],axis=1)\n",
    "\n",
    "val=numeric_val.merge(object_val,left_index=True,right_index=True)\n",
    "val=val.drop(['dt_of_athrztn','fr_yn',\"bldng_cnt_size\",\"road_danger\"],axis=1)\n",
    "\n",
    "test=numeric_test.merge(object_test,left_index=True,right_index=True)\n",
    "test=test.drop(['many','dt_of_athrztn','fr_yn',\"bldng_cnt_size\",\"road_danger\"],axis=1)\n",
    "\n",
    "\n",
    "print(set(train.columns)-set(val.columns))\n",
    "print(set(train.columns)-set(test.columns))\n",
    "print(set(val.columns)-set(train.columns))\n",
    "print(set(val.columns)-set(test.columns))\n",
    "print(set(test.columns)-set(train.columns))\n",
    "print(set(test.columns)-set(val.columns))\n",
    "\n",
    "\n",
    "train=train.rename(columns={\"공동주택\":\"flats\",\"공장\":\"factory\",\"관광휴게시설\":\"tourism_center\",\"교육연구시설\":\"lab\",\"근린생활시설\":\"leisure_center\",\"노유자시설\":\"child\",\"단독주택\":\"single_house\"\\\n",
    "                           ,\"동.식물 관련시설\":\"farm\",\"문화및집회시설\":\"culture_center\",\"분뇨.쓰레기처리시설\":\"cleaning\",\"숙박시설\":\"hotel\",\"업무시설\":\"office\",\"운동시설\":\"gym\",\"위락시설\":\"play_ground\"\\\n",
    "                           ,\"위험물저장및처리시설\":\"danger_facility\",\"의료시설\":\"hospital\",\"자동차관련시설\":\"car\",\"제1종근린생활시설\":\"no_1_leisure\",\"제2종근린생활시설\":\"no_2_leisure\"\\\n",
    "                           ,\"종교시설\":\"relision\",\"창고시설\":\"warehouse\",\"판매시설\":\"shop\"})\n",
    "val=val.rename(columns={\"공동주택\":\"flats\",\"공장\":\"factory\",\"관광휴게시설\":\"tourism_center\",\"교육연구시설\":\"lab\",\"근린생활시설\":\"leisure_center\",\"노유자시설\":\"child\",\"단독주택\":\"single_house\"\\\n",
    "                           ,\"동.식물 관련시설\":\"farm\",\"문화및집회시설\":\"culture_center\",\"분뇨.쓰레기처리시설\":\"cleaning\",\"숙박시설\":\"hotel\",\"업무시설\":\"office\",\"운동시설\":\"gym\",\"위락시설\":\"play_ground\"\\\n",
    "                           ,\"위험물저장및처리시설\":\"danger_facility\",\"의료시설\":\"hospital\",\"자동차관련시설\":\"car\",\"제1종근린생활시설\":\"no_1_leisure\",\"제2종근린생활시설\":\"no_2_leisure\"\\\n",
    "                           ,\"종교시설\":\"relision\",\"창고시설\":\"warehouse\",\"판매시설\":\"shop\"})\n",
    "\n",
    "test=test.rename(columns={\"공동주택\":\"flats\",\"공장\":\"factory\",\"관광휴게시설\":\"tourism_center\",\"교육연구시설\":\"lab\",\"근린생활시설\":\"leisure_center\",\"노유자시설\":\"child\",\"단독주택\":\"single_house\"\\\n",
    "                           ,\"동.식물 관련시설\":\"farm\",\"문화및집회시설\":\"culture_center\",\"분뇨.쓰레기처리시설\":\"cleaning\",\"숙박시설\":\"hotel\",\"업무시설\":\"office\",\"운동시설\":\"gym\",\"위락시설\":\"play_ground\"\\\n",
    "                           ,\"위험물저장및처리시설\":\"danger_facility\",\"의료시설\":\"hospital\",\"자동차관련시설\":\"car\",\"제1종근린생활시설\":\"no_1_leisure\",\"제2종근린생활시설\":\"no_2_leisure\"\\\n",
    "                           ,\"종교시설\":\"relision\",\"창고시설\":\"warehouse\",\"판매시설\":\"shop\"})\n",
    "\n",
    "\n",
    "train.isna().sum()\n",
    "\n",
    "\n",
    "\n",
    "train_comp=train.copy()\n",
    "val_comp=val.copy()\n",
    "test_comp=test.copy()\n",
    "\n",
    "# Train\n",
    "\n",
    "\n",
    "X_train = train.drop('target_value', axis=1)\n",
    "y_train = train['target_value']\n",
    "X_val = val.drop('target_value', axis=1)\n",
    "y_val = val['target_value']\n",
    "test = test\n",
    "\n",
    "\n",
    "df_all = pd.concat([X_train, X_val, test])\n",
    "\n",
    "categorical_cols = df_all.select_dtypes(['object']).columns\n",
    "\n",
    "for col in categorical_cols:\n",
    "    df_all[col] = pd.Categorical(df_all[col]).codes\n",
    "\n",
    "X_train = df_all[:len(train)]\n",
    "X_val = df_all[len(train):-len(test)]\n",
    "test = df_all[-len(test):]\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "imp = IterativeImputer(missing_values=np.nan, sample_posterior=False, \n",
    "                                 max_iter=10, tol=0.001, \n",
    "                                 n_nearest_features=4, initial_strategy='median')\n",
    "\n",
    "imp.fit(X_train)\n",
    "X_train = pd.DataFrame(data=imp.transform(X_train), \n",
    "                             columns=X_train.columns.tolist(),\n",
    "                             dtype='int')\n",
    "\n",
    "imp.fit(X_val)\n",
    "X_val = pd.DataFrame(data=imp.transform(X_val), \n",
    "                             columns=X_val.columns.tolist(),\n",
    "                             dtype='int')\n",
    "imp.fit(test)\n",
    "test = pd.DataFrame(data=imp.transform(test), \n",
    "                             columns=test.columns.tolist(),\n",
    "                             dtype='int')\n",
    "\n",
    "\n",
    "X_train = X_train.fillna(-1)\n",
    "X_val = X_val.fillna(-1)\n",
    "test = test.fillna(-1)\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "# print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()\n",
    "\n",
    "feat_importances.sort_values(ascending=False)\n",
    "\n",
    "중요_변수_리스트=feat_importances[feat_importances>0.001].index.tolist()\n",
    "\n",
    "# 중요_변수_리스트.append(\"Use_T_YN\")\n",
    "# X_train=X_train[중요_변수_리스트]\n",
    "\n",
    "# X_val=X_val[중요_변수_리스트]\n",
    "\n",
    "\n",
    "\n",
    "from  sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_jobs=-1, n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "f1_score(y_val, y_pred)\n",
    "\n",
    "sss = StratifiedKFold(n_splits=5, random_state=None, shuffle=True)\n",
    "\n",
    "for train_index, test_index in sss.split(X_train, y_train):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    original_Xtrain, original_Xtest = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    original_ytrain, original_ytest = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.\n",
    "# original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the Distribution of the labels\n",
    "\n",
    "\n",
    "# Turn into an array\n",
    "original_Xtrain = original_Xtrain.values\n",
    "original_Xtest = original_Xtest.values\n",
    "original_ytrain = original_ytrain.values\n",
    "original_ytest = original_ytest.values\n",
    "\n",
    "\n",
    "original_ytrain=pd.Series(original_ytrain)\n",
    "original_ytest=pd.Series(original_ytest)\n",
    "\n",
    "\n",
    "# X_train.columns = [''] * len(X_train.columns)\n",
    "# y_train.columns = [''] * len(y_train.columns)\n",
    "# X_val.columns = [''] * len(X_val.columns)\n",
    "# y_val.columns = [''] * len(y_val.columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(original_Xtrain, original_ytrain)\n",
    "lgb_eval = lgb.Dataset(original_Xtest, original_ytest, reference=lgb_train)\n",
    "\n",
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_hat), True\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "params = {\n",
    "        'task': 'train',\n",
    "        'objective': 'binary',    \n",
    "        'metric': 'binary_error', \n",
    "        'verbose': 1\n",
    "}\n",
    "\n",
    "clf = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=3000,\n",
    "                valid_sets=lgb_eval,\n",
    "                feval=lgb_f1_score,\n",
    "                evals_result=evals_result,\n",
    "                early_stopping_rounds=10)\n",
    "\n",
    "# clf = lgb.train(params, train_data, valid_sets=[val_data, train_data], valid_names=['val', 'train'], feval=lgb_f1_score, evals_result=evals_result)\n",
    "\n",
    "lgb.plot_metric(evals_result, metric='f1')\n",
    "\n",
    "예측 = clf.predict(X_val, num_iteration=clf.best_iteration)\n",
    "\n",
    "sns.distplot(예측)\n",
    "\n",
    "예측 = clf.predict(X_val, num_iteration=clf.best_iteration)\n",
    "예측[예측>0.49]=1\n",
    "예측[예측<=0.49]=0\n",
    "\n",
    "정답=y_val\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(정답, 예측)\n",
    "print(cm)\n",
    "\n",
    "plt.clf()\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "classNames = ['Negative','Positive']\n",
    "plt.title('Versicolor or Not Versicolor Confusion Matrix - Test Data')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "plt.show()\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "print(sklearn.metrics.recall_score(정답,예측))\n",
    "print(sklearn.metrics.precision_score(정답,예측))\n",
    "print(sklearn.metrics.accuracy_score(정답,예측))\n",
    "print(\"####################################################\")\n",
    "\n",
    "print(\"이게중요!\",sklearn.metrics.f1_score(정답,예측))\n",
    "\n",
    "print(\"####################################################\")\n",
    "\n",
    "print(sklearn.metrics.precision_recall_curve(정답,예측))\n",
    "\n",
    "# 베이지안 옵티미제이션\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "import numpy as np\n",
    "\n",
    "def bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=5, random_seed=6, n_estimators=10000, learning_rate=0.05, output_process=False):\n",
    "    # prepare data\n",
    "    train_data = lgb.Dataset(data=X, label=y, free_raw_data=False)\n",
    "    # parameters\n",
    "    def lgb_eval(num_leaves, feature_fraction, bagging_fraction, lambda_l1, lambda_l2, min_split_gain, min_child_weight):\n",
    "        params = {'application':'binary','num_iterations': n_estimators, 'learning_rate':learning_rate, 'early_stopping_round':100, 'metric':'auc'}\n",
    "        params[\"num_leaves\"] = int(round(num_leaves))\n",
    "        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
    "        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "#         params['max_depth'] = int(round(max_depth))\n",
    "        params['lambda_l1'] = max(lambda_l1, 0)\n",
    "        params['lambda_l2'] = max(lambda_l2, 0)\n",
    "        params['min_split_gain'] = min_split_gain\n",
    "        params['min_child_weight'] = min_child_weight\n",
    "        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'])\n",
    "        return max(cv_result['auc-mean'])\n",
    "    # range \n",
    "    lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (24, 45),\n",
    "                                            'feature_fraction': (0.1, 0.9),\n",
    "                                            'bagging_fraction': (0.6, 1),\n",
    "#                                             'max_depth': (5, 8.99),\n",
    "                                            'lambda_l1': (0, 5),\n",
    "                                            'lambda_l2': (0, 3),\n",
    "                                            'min_split_gain': (1e-5, 0.1),\n",
    "                                            'min_child_weight': (0.5, 50)}, random_state=0)\n",
    "    # optimize\n",
    "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "    \n",
    "    # output optimization process\n",
    "    if output_process==True: lgbBO.points_to_csv(\"bayes_opt_result.csv\")\n",
    "    \n",
    "    # return best parameters\n",
    "    return lgbBO.res\n",
    "\n",
    "opt_params = bayes_parameter_opt_lgb(X_train, y_train, init_round=7, opt_round=20, n_folds=5, random_seed=10, n_estimators=10000, learning_rate=0.05)\n",
    "\n",
    "optimal_params = {v:float(t) for v,t in opt_params[16]['params'].items()}\n",
    "\n",
    "\n",
    "optimal_params\n",
    "\n",
    "optimal_params['num_leaves']=int(round(optimal_params['num_leaves']))\n",
    "# optimal_params['max_depth']=int(round(optimal_params['max_depth']))\n",
    "\n",
    "optimal_params.update({'task': 'train','objective': 'binary','metric': 'binary_error','verbose':1,'max_depth':-1})\n",
    "\n",
    "total_data_X=X_train.append(X_val)\n",
    "\n",
    "total_data_y=y_train.append(y_val)\n",
    "\n",
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
    "\n",
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_hat), True\n",
    "\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "model = lgb.train(optimal_params,\n",
    "                lgb_train,\n",
    "                num_boost_round=1000,\n",
    "                valid_sets=lgb_eval,\n",
    "                feval=lgb_f1_score,\n",
    "                evals_result=evals_result,\n",
    "                early_stopping_rounds=5)\n",
    "\n",
    "\n",
    "\n",
    "# clf = lgb.train(params, train_data, valid_sets=[val_data, train_data], valid_names=['val', 'train'], feval=lgb_f1_score, evals_result=evals_result)\n",
    "\n",
    "# lgb.plot_metric(evals_result, metric='f1')\n",
    "\n",
    "예측 = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "\n",
    "예측 = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "예측[예측>0.49]=1\n",
    "예측[예측<=0.49]=0\n",
    "\n",
    "정답=y_val\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(정답, 예측)\n",
    "print(cm)\n",
    "\n",
    "plt.clf()\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "classNames = ['Negative','Positive']\n",
    "plt.title('Versicolor or Not Versicolor Confusion Matrix - Test Data')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "plt.show()\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "print(sklearn.metrics.recall_score(정답,예측))\n",
    "print(sklearn.metrics.precision_score(정답,예측))\n",
    "print(sklearn.metrics.accuracy_score(정답,예측))\n",
    "print(\"####################################################\")\n",
    "\n",
    "print(\"이게중요!\",sklearn.metrics.f1_score(정답,예측))\n",
    "\n",
    "print(\"####################################################\")\n",
    "\n",
    "print(sklearn.metrics.precision_recall_curve(정답,예측))\n",
    "\n",
    "X_test=test[중요_변수_리스트]\n",
    "\n",
    "예측 = clf.predict(test, num_iteration=clf.best_iteration)\n",
    "예측[예측>0.49]=1\n",
    "예측[예측<=0.49]=0\n",
    "\n",
    "\n",
    "예측=pd.DataFrame(예측,columns=[\"fr_yn\"])\n",
    "\n",
    "binary_y = {0: 'N', 1: 'Y'}\n",
    "\n",
    "예측['fr_yn'] = 예측['fr_yn'].map(binary_y)\n",
    "\n",
    "예측.index.drop()\n",
    "\n",
    "예측['fr_yn'].to_csv(\"화재예측과제_Submission.csv\")\n",
    "\n",
    "예측.to_csv(\"화재예측과제_Submission.csv\")\n",
    "\n",
    "# train엔 있고 val엔 없는 데이터 출력 \n",
    "\n",
    "# vice versa상\", \"sprinkler\"] = 0\n",
    "\n",
    "### us_yn 사용 여부\n",
    "\n",
    "us_yn = pd.get_dummies(object_train[\"us_yn\"])\n",
    "object_train = object_train.join(us_yn)\n",
    "\n",
    "us_yn = pd.get_dummies(object_val[\"us_yn\"])\n",
    "object_val = object_val.join(us_yn)\n",
    "\n",
    "us_yn = pd.get_dummies(object_test[\"us_yn\"])\n",
    "object_test = object_test.join(us_yn)\n",
    "\n",
    "\n",
    "### dngrs_thng_yn 위험물대상여부\n",
    "\n",
    "object_train.loc[object_train[\"dngrs_thng_yn\"].isnull(),\"dngrs_thng\"]= 0\n",
    "object_train.loc[object_train[\"dngrs_thng_yn\"].notnull(),\"dngrs_thng\"]= 1\n",
    "\n",
    "crosstab_ratio(object_train,\"dngrs_thng_yn\")\n",
    "\n",
    "object_val.loc[object_val[\"dngrs_thng_yn\"].isnull(),\"dngrs_thng\"]= 0\n",
    "object_val.loc[object_val[\"dngrs_thng_yn\"].notnull(),\"dngrs_thng\"]= 1\n",
    "\n",
    "object_test.loc[object_test[\"dngrs_thng_yn\"].isnull(),\"dngrs_thng\"]= 0\n",
    "object_test.loc[object_test[\"dngrs_thng_yn\"].notnull(),\"dngrs_thng\"]= 1\n",
    "\n",
    "\n",
    "object_train.drop([\"dt_of_fr\",\"bldng_us\",\"bldng_archtctr\",\"dt_of_athrztn\",\"bldng_us_clssfctn\",\"jmk\",\"rgnl_ar_nm\",\"rgnl_ar_nm2\",\"lnd_us_sttn_nm\",\"rd_sd_nm\",\"emd_nm\",\"mlt_us_yn\",\"trgt_crtr\",\"fr_fghtng_fclt_spcl_css_5_yn\",\"fr_fghtng_fclt_spcl_css_6_yn\"\\\n",
    "\n",
    ",\"us_yn\",\"dngrs_thng_yn\",\"slf_fr_brgd_yn\",\"blk_dngrs_thng_mnfctr_yn\",\"cltrl_hrtg_yn\",\"bldng_archt_danger\",\"Bldng_old_YN\",\"fr_yn\"],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "object_val.drop([\"dt_of_fr\",\"bldng_us\",\"bldng_archtctr\",\"dt_of_athrztn\",\"bldng_us_clssfctn\",\"jmk\",\"rgnl_ar_nm\",\"rgnl_ar_nm2\",\"lnd_us_sttn_nm\",\"rd_sd_nm\",\"emd_nm\",\"mlt_us_yn\",\"trgt_crtr\",\"fr_fghtng_fclt_spcl_css_5_yn\",\"fr_fghtng_fclt_spcl_css_6_yn\"\\\n",
    "\n",
    ",\"us_yn\",\"dngrs_thng_yn\",\"slf_fr_brgd_yn\",\"blk_dngrs_thng_mnfctr_yn\",\"cltrl_hrtg_yn\",\"bldng_archt_danger\",\"Bldng_old_YN\",\"fr_yn\",'교정및군사시설','기타콘크리트구조',\\\n",
    "                '묘지관련시설','방송통신시설','통나무구조'],axis=1,inplace=True)\n",
    "\n",
    "object_test.drop([\"dt_of_fr\",\"bldng_us\",\"bldng_archtctr\",\"dt_of_athrztn\",\"bldng_us_clssfctn\",\"jmk\",\"rgnl_ar_nm\",\"rgnl_ar_nm2\",\"lnd_us_sttn_nm\",\"rd_sd_nm\",\"emd_nm\",\"mlt_us_yn\",\"trgt_crtr\",\"fr_fghtng_fclt_spcl_css_5_yn\",\"fr_fghtng_fclt_spcl_css_6_yn\"\\\n",
    "\n",
    ",\"us_yn\",\"dngrs_thng_yn\",\"slf_fr_brgd_yn\",\"blk_dngrs_thng_mnfctr_yn\",\"cltrl_hrtg_yn\",\"bldng_archt_danger\",\"Bldng_old_YN\",'기타콘크리트구조',],axis=1,inplace=True)\n",
    "\n",
    "print(set(object_test.columns)-set(object_train.columns))\n",
    "print(set(object_test.columns)-set(object_val.columns))\n",
    "print(set(object_train.columns)-set(object_test.columns))\n",
    "print(set(object_train.columns)-set(object_val.columns))\n",
    "print(set(object_val.columns)-set(object_train.columns))\n",
    "print(set(object_val.columns)-set(object_test.columns))\n",
    "\n",
    "\n",
    "# 수치형 데이터 따로 정리\n",
    "\n",
    "numeric_list=list(set(train.columns.tolist()) - set(object_list))\n",
    "\n",
    "numeric_list=sorted(numeric_list)\n",
    "\n",
    "numeric_train=train[numeric_list[:]]\n",
    "\n",
    "\n",
    "numeric_list=list(set(val.columns.tolist()) - set(object_list))\n",
    "\n",
    "numeric_list=sorted(numeric_list)\n",
    "\n",
    "numeric_val=val[numeric_list[:]]\n",
    "\n",
    "\n",
    "\n",
    "numeric_list=list(set(test.columns.tolist()) - set(object_list))\n",
    "\n",
    "numeric_list=sorted(numeric_list)\n",
    "\n",
    "numeric_test=test[numeric_list[:]]\n",
    "\n",
    "numeric_train\n",
    "\n",
    "### bldng_cnt 건물채수\n",
    "\n",
    "small_crit=11\n",
    "middle_crit=74\n",
    "\n",
    "numeric_train.loc[numeric_train[\"bldng_cnt\"]<small_crit, \"bldng_cnt_size\"] = \"few\"\n",
    "numeric_train.loc[(numeric_train[\"bldng_cnt\"]>=small_crit)&(numeric_train[\"bldng_cnt\"]<middle_crit), \"bldng_cnt_size\"] = \"soso\"\n",
    "numeric_train.loc[numeric_train[\"bldng_cnt\"]>=middle_crit, \"bldng_cnt_size\"] = \"many\"\n",
    "\n",
    "small_crit=11\n",
    "middle_crit=74\n",
    "\n",
    "numeric_train.loc[train[\"bldng_cnt\"]<small_crit, \"bldng_cnt_encoded\"] = \"small\"\n",
    "numeric_train.loc[(train[\"bldng_cnt\"]>=small_crit)&(numeric_train[\"bldng_cnt\"]<middle_crit), \"bldng_cnt_encoded\"] = \"middle\"\n",
    "numeric_train.loc[train[\"bldng_cnt\"]>=middle_crit, \"bldng_cnt_encoded\"] = \"big\"\n",
    "\n",
    "crosstab_ratio(numeric_train,\"bldng_cnt_encoded\")\n",
    "\n",
    "one_hot_bldng_cnt = pd.get_dummies(numeric_train[\"bldng_cnt_size\"])\n",
    "numeric_train = numeric_train.join(one_hot_bldng_cnt)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "numeric_val.loc[numeric_val[\"bldng_cnt\"]<small_crit, \"bldng_cnt_size\"] = \"few\"\n",
    "numeric_val.loc[(numeric_val[\"bldng_cnt\"]>=small_crit)&(numeric_val[\"bldng_cnt\"]<middle_crit), \"bldng_cnt_size\"] = \"soso\"\n",
    "numeric_val.loc[numeric_val[\"bldng_cnt\"]>=middle_crit, \"bldng_cnt_size\"] = \"many\"\n",
    "\n",
    "one_hot_bldng_cnt = pd.get_dummies(numeric_val[\"bldng_cnt_size\"])\n",
    "numeric_val = numeric_val.join(one_hot_bldng_cnt)\n",
    "\n",
    "\n",
    "\n",
    "numeric_test.loc[numeric_test[\"bldng_cnt\"]<small_crit, \"bldng_cnt_size\"] = \"few\"\n",
    "numeric_test.loc[(numeric_test[\"bldng_cnt\"]>=small_crit)&(numeric_test[\"bldng_cnt\"]<middle_crit), \"bldng_cnt_size\"] = \"soso\"\n",
    "numeric_test.loc[numeric_test[\"bldng_cnt\"]>=middle_crit, \"bldng_cnt_size\"] = \"many\"\n",
    "\n",
    "one_hot_bldng_cnt = pd.get_dummies(numeric_test[\"bldng_cnt_size\"])\n",
    "numeric_test = numeric_test.join(one_hot_bldng_cnt)\n",
    "\n",
    "### bldng_ar & ttl_ar & lnd_ar 건물건축면적, 건물연면적, 토지면적\n",
    "\n",
    "# val 값이 전처리 하기 전이 조금 더 잘나옴\n",
    "\n",
    "train[\"bldng_ar\"].head(10)\n",
    "\n",
    "\n",
    "\n",
    "bldng_ar_notnull = train[train[\"bldng_ar\"]!=0]\n",
    "sns.lmplot(data=bldng_ar_notnull, x=\"bldng_ar\", y=\"ttl_ar\", hue=\"fr_yn\", fit_reg=False)\n",
    "\n",
    "low_bldng_ar = train[(train[\"bldng_ar\"]<4000)&(train[\"bldng_ar\"]>0)&(train[\"ttl_ar\"]<3000)&(train[\"ttl_ar\"]>0)]\n",
    "low_bldng_ar.shape\n",
    "\n",
    "sns.lmplot(data=low_bldng_ar, x=\"bldng_ar\", y=\"ttl_ar\", hue=\"fr_yn\", fit_reg=False)\n",
    "\n",
    "low_bldng_ar = train[(train[\"bldng_ar\"]<4000)&(train[\"bldng_ar\"]>0)\n",
    "                     &(train[\"ttl_ar\"]<3000)&(train[\"ttl_ar\"]>0)\n",
    "                     &(train[\"lnd_ar\"]<4000)&(train[\"lnd_ar\"]>0)]\n",
    "\n",
    "sns.lmplot(data=low_bldng_ar, x=\"bldng_ar\", y=\"lnd_ar\", hue=\"fr_yn\", fit_reg=False, size=10)\n",
    "\n",
    "sns.lmplot(data=low_bldng_ar, x=\"ttl_ar\", y=\"lnd_ar\", hue=\"fr_yn\", fit_reg=False, size=10)\n",
    "\n",
    "low_low_bldng_ar = train[(train[\"bldng_ar\"]<1500)&(train[\"bldng_ar\"]>0)\n",
    "                     &(train[\"ttl_ar\"]<2000)&(train[\"ttl_ar\"]>0)\n",
    "                     &(train[\"lnd_ar\"]<2000)&(train[\"lnd_ar\"]>0)]\n",
    "low_low_bldng_ar.shape\n",
    "\n",
    "sns.lmplot(data=low_low_bldng_ar, x=\"bldng_ar\", y=\"ttl_ar\", hue=\"fr_yn\", fit_reg=False, size=10)\n",
    "\n",
    "train[train[\"bldng_ar\"]==0].shape\n",
    "\n",
    "train[\"bldng_ar\"].describe()\n",
    "\n",
    "train[\"ttl_ar\"].mean()\n",
    "\n",
    "train[\"ttl_ar\"].median()\n",
    "\n",
    "train.loc[train[\"bldng_ar\"]==0, \"bldng_ar\"] = train[\"bldng_ar\"].median()\n",
    "train.loc[train[\"ttl_ar\"]==0, \"ttl_ar\"] = train[\"ttl_ar\"].median()\n",
    "\n",
    "val.loc[val[\"bldng_ar\"]==0, \"bldng_ar\"] = val[\"bldng_ar\"].median()\n",
    "val.loc[val[\"ttl_ar\"]==0, \"ttl_ar\"] = val[\"ttl_ar\"].median()\n",
    "\n",
    "test.loc[test[\"bldng_ar\"]==0, \"bldng_ar\"] = test[\"bldng_ar\"].median()\n",
    "test.loc[test[\"ttl_ar\"]==0, \"ttl_ar\"] = test[\"ttl_ar\"].median()\n",
    "\n",
    "\n",
    "#train[\"floors\"] = train[\"ttl_ar\"] / train[\"bldng_ar\"]\n",
    "#train[\"floors\"].head()\n",
    "\n",
    "train = train.drop(\"lnd_ar\", 1)\n",
    "val = val.drop(\"lnd_ar\", 1)\n",
    "test = test.drop(\"lnd_ar\", 1)\n",
    "\n",
    "## part 3\n",
    "\n",
    "### ele_energy_us_YYYYMM 전기 에너지 사용량 (YYYY년 M월)\n",
    "\n",
    "### slf_fr_brgd_yn 자체소방대여부\n",
    "\n",
    "### blk_dngrs_thng_mnfctr_yn 대량위험물제조소등여부\n",
    "\n",
    "### cltrl_hrtg_yn 문화재여부\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train=numeric_train.merge(object_train,left_index=True,right_index=True)\n",
    "train=train.drop(['many','dt_of_athrztn','fr_yn',\"bldng_cnt_size\",\"road_danger\"],axis=1)\n",
    "\n",
    "val=numeric_val.merge(object_val,left_index=True,right_index=True)\n",
    "val=val.drop(['dt_of_athrztn','fr_yn',\"bldng_cnt_size\",\"road_danger\"],axis=1)\n",
    "\n",
    "test=numeric_test.merge(object_test,left_index=True,right_index=True)\n",
    "test=test.drop(['many','dt_of_athrztn','fr_yn',\"bldng_cnt_size\",\"road_danger\"],axis=1)\n",
    "\n",
    "\n",
    "print(set(train.columns)-set(val.columns))\n",
    "print(set(train.columns)-set(test.columns))\n",
    "print(set(val.columns)-set(train.columns))\n",
    "print(set(val.columns)-set(test.columns))\n",
    "print(set(test.columns)-set(train.columns))\n",
    "print(set(test.columns)-set(val.columns))\n",
    "\n",
    "\n",
    "train=train.rename(columns={\"공동주택\":\"flats\",\"공장\":\"factory\",\"관광휴게시설\":\"tourism_center\",\"교육연구시설\":\"lab\",\"근린생활시설\":\"leisure_center\",\"노유자시설\":\"child\",\"단독주택\":\"single_house\"\\\n",
    "                           ,\"동.식물 관련시설\":\"farm\",\"문화및집회시설\":\"culture_center\",\"분뇨.쓰레기처리시설\":\"cleaning\",\"숙박시설\":\"hotel\",\"업무시설\":\"office\",\"운동시설\":\"gym\",\"위락시설\":\"play_ground\"\\\n",
    "                           ,\"위험물저장및처리시설\":\"danger_facility\",\"의료시설\":\"hospital\",\"자동차관련시설\":\"car\",\"제1종근린생활시설\":\"no_1_leisure\",\"제2종근린생활시설\":\"no_2_leisure\"\\\n",
    "                           ,\"종교시설\":\"relision\",\"창고시설\":\"warehouse\",\"판매시설\":\"shop\"})\n",
    "val=val.rename(columns={\"공동주택\":\"flats\",\"공장\":\"factory\",\"관광휴게시설\":\"tourism_center\",\"교육연구시설\":\"lab\",\"근린생활시설\":\"leisure_center\",\"노유자시설\":\"child\",\"단독주택\":\"single_house\"\\\n",
    "                           ,\"동.식물 관련시설\":\"farm\",\"문화및집회시설\":\"culture_center\",\"분뇨.쓰레기처리시설\":\"cleaning\",\"숙박시설\":\"hotel\",\"업무시설\":\"office\",\"운동시설\":\"gym\",\"위락시설\":\"play_ground\"\\\n",
    "                           ,\"위험물저장및처리시설\":\"danger_facility\",\"의료시설\":\"hospital\",\"자동차관련시설\":\"car\",\"제1종근린생활시설\":\"no_1_leisure\",\"제2종근린생활시설\":\"no_2_leisure\"\\\n",
    "                           ,\"종교시설\":\"relision\",\"창고시설\":\"warehouse\",\"판매시설\":\"shop\"})\n",
    "\n",
    "test=test.rename(columns={\"공동주택\":\"flats\",\"공장\":\"factory\",\"관광휴게시설\":\"tourism_center\",\"교육연구시설\":\"lab\",\"근린생활시설\":\"leisure_center\",\"노유자시설\":\"child\",\"단독주택\":\"single_house\"\\\n",
    "                           ,\"동.식물 관련시설\":\"farm\",\"문화및집회시설\":\"culture_center\",\"분뇨.쓰레기처리시설\":\"cleaning\",\"숙박시설\":\"hotel\",\"업무시설\":\"office\",\"운동시설\":\"gym\",\"위락시설\":\"play_ground\"\\\n",
    "                           ,\"위험물저장및처리시설\":\"danger_facility\",\"의료시설\":\"hospital\",\"자동차관련시설\":\"car\",\"제1종근린생활시설\":\"no_1_leisure\",\"제2종근린생활시설\":\"no_2_leisure\"\\\n",
    "                           ,\"종교시설\":\"relision\",\"창고시설\":\"warehouse\",\"판매시설\":\"shop\"})\n",
    "\n",
    "\n",
    "train.isna().sum()\n",
    "\n",
    "\n",
    "\n",
    "train_comp=train.copy()\n",
    "val_comp=val.copy()\n",
    "test_comp=test.copy()\n",
    "\n",
    "# Train\n",
    "\n",
    "\n",
    "X_train = train.drop('target_value', axis=1)\n",
    "y_train = train['target_value']\n",
    "X_val = val.drop('target_value', axis=1)\n",
    "y_val = val['target_value']\n",
    "test = test\n",
    "\n",
    "\n",
    "df_all = pd.concat([X_train, X_val, test])\n",
    "\n",
    "categorical_cols = df_all.select_dtypes(['object']).columns\n",
    "\n",
    "for col in categorical_cols:\n",
    "    df_all[col] = pd.Categorical(df_all[col]).codes\n",
    "\n",
    "X_train = df_all[:len(train)]\n",
    "X_val = df_all[len(train):-len(test)]\n",
    "test = df_all[-len(test):]\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "imp = IterativeImputer(missing_values=np.nan, sample_posterior=False, \n",
    "                                 max_iter=10, tol=0.001, \n",
    "                                 n_nearest_features=4, initial_strategy='median')\n",
    "\n",
    "imp.fit(X_train)\n",
    "X_train = pd.DataFrame(data=imp.transform(X_train), \n",
    "                             columns=X_train.columns.tolist(),\n",
    "                             dtype='int')\n",
    "\n",
    "imp.fit(X_val)\n",
    "X_val = pd.DataFrame(data=imp.transform(X_val), \n",
    "                             columns=X_val.columns.tolist(),\n",
    "                             dtype='int')\n",
    "imp.fit(test)\n",
    "test = pd.DataFrame(data=imp.transform(test), \n",
    "                             columns=test.columns.tolist(),\n",
    "                             dtype='int')\n",
    "\n",
    "\n",
    "X_train = X_train.fillna(-1)\n",
    "X_val = X_val.fillna(-1)\n",
    "test = test.fillna(-1)\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X_train,y_train)\n",
    "# print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X_train.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()\n",
    "\n",
    "feat_importances.sort_values(ascending=False)\n",
    "\n",
    "중요_변수_리스트=feat_importances[feat_importances>0.001].index.tolist()\n",
    "\n",
    "# 중요_변수_리스트.append(\"Use_T_YN\")\n",
    "# X_train=X_train[중요_변수_리스트]\n",
    "\n",
    "# X_val=X_val[중요_변수_리스트]\n",
    "\n",
    "\n",
    "\n",
    "from  sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_jobs=-1, n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "f1_score(y_val, y_pred)\n",
    "\n",
    "sss = StratifiedKFold(n_splits=5, random_state=None, shuffle=True)\n",
    "\n",
    "for train_index, test_index in sss.split(X_train, y_train):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    original_Xtrain, original_Xtest = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    original_ytrain, original_ytest = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "\n",
    "# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.\n",
    "# original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the Distribution of the labels\n",
    "\n",
    "\n",
    "# Turn into an array\n",
    "original_Xtrain = original_Xtrain.values\n",
    "original_Xtest = original_Xtest.values\n",
    "original_ytrain = original_ytrain.values\n",
    "original_ytest = original_ytest.values\n",
    "\n",
    "\n",
    "original_ytrain=pd.Series(original_ytrain)\n",
    "original_ytest=pd.Series(original_ytest)\n",
    "\n",
    "\n",
    "# X_train.columns = [''] * len(X_train.columns)\n",
    "# y_train.columns = [''] * len(y_train.columns)\n",
    "# X_val.columns = [''] * len(X_val.columns)\n",
    "# y_val.columns = [''] * len(y_val.columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# from sklearn.metrics import f1_score\n",
    "\n",
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(original_Xtrain, original_ytrain)\n",
    "lgb_eval = lgb.Dataset(original_Xtest, original_ytest, reference=lgb_train)\n",
    "\n",
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_hat), True\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "params = {\n",
    "        'task': 'train',\n",
    "        'objective': 'binary',    \n",
    "        'metric': 'binary_error', \n",
    "        'verbose': 1\n",
    "}\n",
    "\n",
    "clf = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=3000,\n",
    "                valid_sets=lgb_eval,\n",
    "                feval=lgb_f1_score,\n",
    "                evals_result=evals_result,\n",
    "                early_stopping_rounds=10)\n",
    "\n",
    "# clf = lgb.train(params, train_data, valid_sets=[val_data, train_data], valid_names=['val', 'train'], feval=lgb_f1_score, evals_result=evals_result)\n",
    "\n",
    "lgb.plot_metric(evals_result, metric='f1')\n",
    "\n",
    "예측 = clf.predict(X_val, num_iteration=clf.best_iteration)\n",
    "\n",
    "sns.distplot(예측)\n",
    "\n",
    "예측 = clf.predict(X_val, num_iteration=clf.best_iteration)\n",
    "예측[예측>0.49]=1\n",
    "예측[예측<=0.49]=0\n",
    "\n",
    "정답=y_val\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(정답, 예측)\n",
    "print(cm)\n",
    "\n",
    "plt.clf()\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "classNames = ['Negative','Positive']\n",
    "plt.title('Versicolor or Not Versicolor Confusion Matrix - Test Data')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "plt.show()\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "print(sklearn.metrics.recall_score(정답,예측))\n",
    "print(sklearn.metrics.precision_score(정답,예측))\n",
    "print(sklearn.metrics.accuracy_score(정답,예측))\n",
    "print(\"####################################################\")\n",
    "\n",
    "print(\"이게중요!\",sklearn.metrics.f1_score(정답,예측))\n",
    "\n",
    "print(\"####################################################\")\n",
    "\n",
    "print(sklearn.metrics.precision_recall_curve(정답,예측))\n",
    "\n",
    "# 베이지안 옵티미제이션\n",
    "\n",
    "from bayes_opt import BayesianOptimization\n",
    "import numpy as np\n",
    "\n",
    "def bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=5, random_seed=6, n_estimators=10000, learning_rate=0.05, output_process=False):\n",
    "    # prepare data\n",
    "    train_data = lgb.Dataset(data=X, label=y, free_raw_data=False)\n",
    "    # parameters\n",
    "    def lgb_eval(num_leaves, feature_fraction, bagging_fraction, lambda_l1, lambda_l2, min_split_gain, min_child_weight):\n",
    "        params = {'application':'binary','num_iterations': n_estimators, 'learning_rate':learning_rate, 'early_stopping_round':100, 'metric':'auc'}\n",
    "        params[\"num_leaves\"] = int(round(num_leaves))\n",
    "        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
    "        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "#         params['max_depth'] = int(round(max_depth))\n",
    "        params['lambda_l1'] = max(lambda_l1, 0)\n",
    "        params['lambda_l2'] = max(lambda_l2, 0)\n",
    "        params['min_split_gain'] = min_split_gain\n",
    "        params['min_child_weight'] = min_child_weight\n",
    "        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'])\n",
    "        return max(cv_result['auc-mean'])\n",
    "    # range \n",
    "    lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (24, 45),\n",
    "                                            'feature_fraction': (0.1, 0.9),\n",
    "                                            'bagging_fraction': (0.6, 1),\n",
    "#                                             'max_depth': (5, 8.99),\n",
    "                                            'lambda_l1': (0, 5),\n",
    "                                            'lambda_l2': (0, 3),\n",
    "                                            'min_split_gain': (1e-5, 0.1),\n",
    "                                            'min_child_weight': (0.5, 50)}, random_state=0)\n",
    "    # optimize\n",
    "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "    \n",
    "    # output optimization process\n",
    "    if output_process==True: lgbBO.points_to_csv(\"bayes_opt_result.csv\")\n",
    "    \n",
    "    # return best parameters\n",
    "    return lgbBO.res\n",
    "\n",
    "opt_params = bayes_parameter_opt_lgb(X_train, y_train, init_round=7, opt_round=20, n_folds=5, random_seed=10, n_estimators=10000, learning_rate=0.05)\n",
    "\n",
    "optimal_params = {v:float(t) for v,t in opt_params[16]['params'].items()}\n",
    "\n",
    "\n",
    "optimal_params\n",
    "\n",
    "optimal_params['num_leaves']=int(round(optimal_params['num_leaves']))\n",
    "# optimal_params['max_depth']=int(round(optimal_params['max_depth']))\n",
    "\n",
    "optimal_params.update({'task': 'train','objective': 'binary','metric': 'binary_error','verbose':1,'max_depth':-1})\n",
    "\n",
    "total_data_X=X_train.append(X_val)\n",
    "\n",
    "total_data_y=y_train.append(y_val)\n",
    "\n",
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
    "\n",
    "def lgb_f1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_hat), True\n",
    "\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "model = lgb.train(optimal_params,\n",
    "                lgb_train,\n",
    "                num_boost_round=1000,\n",
    "                valid_sets=lgb_eval,\n",
    "                feval=lgb_f1_score,\n",
    "                evals_result=evals_result,\n",
    "                early_stopping_rounds=5)\n",
    "\n",
    "\n",
    "\n",
    "# clf = lgb.train(params, train_data, valid_sets=[val_data, train_data], valid_names=['val', 'train'], feval=lgb_f1_score, evals_result=evals_result)\n",
    "\n",
    "# lgb.plot_metric(evals_result, metric='f1')\n",
    "\n",
    "예측 = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "\n",
    "예측 = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "예측[예측>0.49]=1\n",
    "예측[예측<=0.49]=0\n",
    "\n",
    "정답=y_val\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(정답, 예측)\n",
    "print(cm)\n",
    "\n",
    "plt.clf()\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "classNames = ['Negative','Positive']\n",
    "plt.title('Versicolor or Not Versicolor Confusion Matrix - Test Data')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "plt.show()\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "print(sklearn.metrics.recall_score(정답,예측))\n",
    "print(sklearn.metrics.precision_score(정답,예측))\n",
    "print(sklearn.metrics.accuracy_score(정답,예측))\n",
    "print(\"####################################################\")\n",
    "\n",
    "print(\"이게중요!\",sklearn.metrics.f1_score(정답,예측))\n",
    "\n",
    "print(\"####################################################\")\n",
    "\n",
    "print(sklearn.metrics.precision_recall_curve(정답,예측))\n",
    "\n",
    "X_test=test[중요_변수_리스트]\n",
    "\n",
    "예측 = clf.predict(test, num_iteration=clf.best_iteration)\n",
    "예측[예측>0.49]=1\n",
    "예측[예측<=0.49]=0\n",
    "\n",
    "\n",
    "예측=pd.DataFrame(예측,columns=[\"fr_yn\"])\n",
    "\n",
    "binary_y = {0: 'N', 1: 'Y'}\n",
    "\n",
    "예측['fr_yn'] = 예측['fr_yn'].map(binary_y)\n",
    "\n",
    "예측.index.drop()\n",
    "\n",
    "예측['fr_yn'].to_csv(\"화재예측과제_Submission.csv\")\n",
    "\n",
    "예측.to_csv(\"화재예측과제_Submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
