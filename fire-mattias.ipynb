{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\ndata = pd.read_csv(\"../input/PJT002_train.csv\", encoding=\"UTF-8-sig\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport sklearn as sk\nimport warnings\nimport itertools as IT\nimport glob\nimport datetime as dt\nimport time\nimport sys\n\nfrom dateutil.relativedelta import relativedelta\n\n# from xgboost import plot_importance\nfrom sklearn.model_selection import cross_val_score,KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV   #Perforing grid search\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.preprocessing import RobustScaler\n\n\n# #Common Model Algorithms\n# from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n# import xgboost\n\n#Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\n\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_rows',100)\npd.set_option('display.max_columns',500)\npd.set_option('display.width',500)\npd.set_option('max_info_columns',500)\npd.set_option('display.float_format', lambda x : '%.3f' % x )\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport collections\n# Other Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\n#from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n#from imblearn.over_sampling import SMOTE\n#from imblearn.under_sampling import NearMiss\n#from imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport collections\n# Other Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n# Our data is already scaled we should split our training and test sets\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\n\n# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV\nimport xgboost\nfrom sklearn.model_selection import cross_val_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.shape\n\nprint('NON FIRE', round(data['fr_yn'].value_counts()[0]/len(data) * 100,2), '% of the dataset')\nprint('FIRE', round(data['fr_yn'].value_counts()[1]/len(data) * 100,2), '% of the dataset')\n\ndata.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 오브젝트 먼저 전처리"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.iloc[:,2].dtype=='O'\n\nobject_list=[]\nfor col in data:\n    if data[col].dtype ==\"object\":\n        object_list.append(col)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 수치형 데이터 따로 정리\n\nobject_data=data[object_list[:]]\n\nnumeric_list=list(set(data.columns.tolist()) - set(object_list))\nnumeric_list=sorted(numeric_list)\nnumeric_data=data[numeric_list[:]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 화재 발생 시기 데이터 타입 변경\nobject_data['dt_of_fr']=object_data['dt_of_fr'].astype('datetime64[ns]')\n\n### bldng_us(사용용도)\n\nobject_data[\"주거용YN\"]=object_data.lnd_us_sttn_nm.apply(lambda x: 1 if x ==\"단독\" else x)\nobject_data[\"주거용YN\"]=object_data.주거용YN.apply(lambda x: 1 if x ==\"연립\" else x)\nobject_data[\"주거용YN\"]=object_data.주거용YN.apply(lambda x: 1 if x ==\"아파트\" else x)\nobject_data[\"주거용YN\"]=object_data.주거용YN.apply(lambda x: 1 if x ==\"다세대\" else x)\nobject_data[\"주거용YN\"]=object_data.주거용YN.apply(lambda x: 1 if x ==\"주거기타\" else x)\nobject_data['주거용YN']=object_data.주거용YN.apply(lambda x: 1 if x==1 else 0)\n\n\nobject_data[\"공업용YN\"]=object_data.lnd_us_sttn_nm.apply(lambda x: 1 if x ==\"공업용\" else x)\nobject_data[\"공업용YN\"]=object_data.공업용YN.apply(lambda x: 1 if x ==\"공업기타\" else x)\nobject_data[\"공업용YN\"]=object_data.공업용YN.apply(lambda x: 1 if x ==\"발전소\" else x)\nobject_data[\"공업용YN\"]=object_data.공업용YN.apply(lambda x: 1 if x ==\"유해.혐오시설\" else x)\nobject_data[\"공업용YN\"]=object_data.공업용YN.apply(lambda x: 1 if x ==\"공업나지\" else x)\nobject_data['공업용YN']=object_data.공업용YN.apply(lambda x: 1 if x==1 else 0)\n\n\nobject_data[\"농업용YN\"]=object_data.lnd_us_sttn_nm.apply(lambda x: 1 if x ==\"자연림\" else x)\nobject_data[\"농업용YN\"]=object_data.농업용YN.apply(lambda x: 1 if x ==\"답\" else x)\nobject_data[\"농업용YN\"]=object_data.농업용YN.apply(lambda x: 1 if x ==\"전\" else x)\nobject_data[\"농업용YN\"]=object_data.농업용YN.apply(lambda x: 1 if x ==\"조림\" else x)\nobject_data[\"농업용YN\"]=object_data.농업용YN.apply(lambda x: 1 if x ==\"답기타\" else x)\nobject_data[\"농업용YN\"]=object_data.농업용YN.apply(lambda x: 1 if x ==\"과수원\" else x)\nobject_data[\"농업용YN\"]=object_data.농업용YN.apply(lambda x: 1 if x ==\"토지임야\" else x)\nobject_data[\"농업용YN\"]=object_data.농업용YN.apply(lambda x: 1 if x ==\"임야기타\" else x)\nobject_data[\"농업용YN\"]=object_data.농업용YN.apply(lambda x: 1 if x ==\"목장용지\" else x)\nobject_data['농업용YN']=object_data.농업용YN.apply(lambda x: 1 if x==1 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 사용 용도에 따라 주거시설/편의시설 여부로 더미화","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"object_data.bldng_us.unique()\n\nobject_data['주거시설YN']=object_data.bldng_us.apply(lambda x: 1 if x==\"단독주택\" else x)\nobject_data['주거시설YN']=object_data.주거시설YN.apply(lambda x: 1 if x==\"공동주택\" else x)\nobject_data['주거시설YN']=object_data.주거시설YN.apply(lambda x: 1 if x==1 else 0)\n\nobject_data['편의시설YN']=object_data.bldng_us.apply(lambda x: 1 if x==\"제1종근린생활시설\" else x)\nobject_data['편의시설YN']=object_data.편의시설YN.apply(lambda x: 1 if x==\"제2종근린생활시설\" else x)\nobject_data['편의시설YN']=object_data.편의시설YN.apply(lambda x: 1 if x==\"문화및집회시설\" else x)\nobject_data['편의시설YN']=object_data.편의시설YN.apply(lambda x: 1 if x==1 else 0)\n\nobject_data.groupby(\"bldng_us\").size().sort_values(ascending=False)\n\n# 6자리 혹은 nan 혹은 10자리임을 알 수 있음.\n#  sns.distplot(object_data.dt_of_athrztn.astype(str).apply(len))\n\n### dt_of_athrztn(건물승인일자)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 건축 날짜의 데이터 타입이 2종류, 따라서 이 모양을 연도 형식으로 바꾸어줄 필요가 있음.\n\nobject_data.dt_of_athrztn=object_data.dt_of_athrztn.apply(lambda x: str(x)[0:4] if len(str(x))==10 else x)\nobject_data.dt_of_athrztn=object_data.dt_of_athrztn.apply(lambda x: str(x)[0:4] if len(str(x))==8 else x)\nobject_data.dt_of_athrztn=object_data.dt_of_athrztn.apply(lambda x: str(x)[0:4] if len(str(x))==6 else x)\n\ndirty_date_idx=object_data.dt_of_athrztn[object_data.dt_of_athrztn.astype(str).apply(len)>4].index\n\nobject_data.dt_of_athrztn[dirty_date_idx[0]]=\"1980\"\nobject_data.dt_of_athrztn[dirty_date_idx[1]]=\"1890\"\nobject_data.dt_of_athrztn[dirty_date_idx[2]]=\"1890\"\nobject_data.dt_of_athrztn[dirty_date_idx[3]]=\"1990\"\nobject_data.dt_of_athrztn[dirty_date_idx[4]]=\"1990\"\nobject_data.dt_of_athrztn[dirty_date_idx[5]]=\"1982\"\nobject_data.dt_of_athrztn[dirty_date_idx[6]]=\"1978\"\nobject_data.dt_of_athrztn[dirty_date_idx[7]]=\"1994\"\n\n#결측치가 아닌 연도 데이터를 시각화 하는 코드\n\n#sns.distplot(object_data.dt_of_athrztn[~(object_data.dt_of_athrztn.isna)()].astype(int))\n\n말도안돼_연도=object_data.dt_of_athrztn[~(object_data.dt_of_athrztn.isna)()].astype(int)[object_data.dt_of_athrztn[~(object_data.dt_of_athrztn.isna)()].astype(int)>2019].index\n\nobject_data.dt_of_athrztn[말도안돼_연도]=\"1997\"\n\n있는_연도=object_data.dt_of_athrztn[~(object_data.dt_of_athrztn.isna())].astype(int)\n\nobject_data.dt_of_athrztn.fillna(\"1988\",inplace=True)\n\nobject_data.dt_of_athrztn=object_data.dt_of_athrztn.astype(int)\n\nobject_data[\"건물연령\"]=object_data.dt_of_athrztn.apply(lambda x: 2019-x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### rd_sd_nm(도로측면명)"},{"metadata":{"trusted":true},"cell_type":"code","source":"object_data.rd_sd_nm.unique()\n\ndata.isnull().sum(axis=1)[data.isnull().sum(axis=1)<150].index\n\nobject_data['차량통행YN']=object_data.rd_sd_nm.apply(lambda x: 1 if x==\"세로한면(가)\" else x)\nobject_data['차량통행YN']=object_data.차량통행YN.apply(lambda x: 1 if x==\"세로각지(가)\" else x)\nobject_data['차량통행YN']=object_data.차량통행YN.apply(lambda x: 1 if x==\"소로각지\" else x)\nobject_data['차량통행YN']=object_data.차량통행YN.apply(lambda x: 1 if x==\"중로한면\" else x)\nobject_data['차량통행YN']=object_data.차량통행YN.apply(lambda x: 1 if x==\"중로각지\" else x)\nobject_data['차량통행YN']=object_data.차량통행YN.apply(lambda x: 1 if x==\"소로한면\" else x)\nobject_data['차량통행YN']=object_data.차량통행YN.apply(lambda x: 1 if x==1 else 0)\n\nobject_data['대로인접YN']=object_data.rd_sd_nm.apply(lambda x: 1 if x==\"중로각지\" else x)\nobject_data['대로인접YN']=object_data.대로인접YN.apply(lambda x: 1 if x==\"중로한면\" else x)\nobject_data['대로인접YN']=object_data.대로인접YN.apply(lambda x: 1 if x==\"광대세각\" else x)\nobject_data['대로인접YN']=object_data.대로인접YN.apply(lambda x: 1 if x==\"광대소각\" else x)\nobject_data['대로인접YN']=object_data.대로인접YN.apply(lambda x: 1 if x==\"광대로한면\" else x)\nobject_data['대로인접YN']=object_data.대로인접YN.apply(lambda x: 1 if x==1 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### bldng_archtctr(건물구조)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"object_data['목재구조YN']=object_data.bldng_archtctr.apply(lambda x: 1 if x==\"일반목구조\" else x)\nobject_data['목재구조YN']=object_data.목재구조YN.apply(lambda x: 1 if x==\"통나무구조\" else x)\nobject_data['목재구조YN']=object_data.목재구조YN.apply(lambda x: 1 if x==\"목구조\" else x)\nobject_data['목재구조YN']=object_data.목재구조YN.apply(lambda x: 1 if x==1 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### bldng_us_clssfctn(건물용도분류명)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### rgnl_ar_nm(용도지역지구명)"},{"metadata":{"trusted":true},"cell_type":"code","source":"object_data.rgnl_ar_nm.unique()\n\nobject_data.groupby(\"rgnl_ar_nm\").size().sort_values(ascending=False)\n\nobject_data.bldng_us_clssfctn.unique()\n\nobject_data.groupby(\"bldng_us_clssfctn\").size().sort_values(ascending=False)\n\nobject_data['주거용YN']=object_data.bldng_us_clssfctn.apply(lambda x: 1 if x==\"주거용\" else 0)\n\nobject_data['target_value']=object_data.fr_yn.apply(lambda x: 1 if x==\"Y\" else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### emd_nm(행정구역명) 데이터"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(object_data.emd_nm.astype(str).apply(len))\n\nobject_data.emd_nm[object_data.emd_nm.astype(str).apply(len)[object_data.emd_nm.astype(str).apply(len)<4].index]\n\n\nobject_data.emd_nm=object_data.emd_nm.str[4:]\n\nobject_data.emd_nm[object_data.emd_nm.notnull()]\n\nobject_data.emd_nm[object_data.emd_nm.isnull()]\n\nobject_data[\"시YN\"]=object_data.emd_nm[object_data.emd_nm.notnull()].apply(lambda x: \"1\" if x[3]==\"시\" else \"0\")\n\n\n\nobject_data.groupby(\"emd_nm\").size().sort_values(ascending=False).sample(100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# 수치형 데이터"},{"metadata":{"trusted":true},"cell_type":"code","source":"numeric_list_not_engry=[s for s in numeric_list if not \"engry\" in s]\n\n\nnumeric_data_on=numeric_data.loc[:,numeric_list_not_engry]\n\nnumeric_data_on.describe()\n\nfor col in numeric_data_on:\n    try:\n        if data[col].dtype !='object':\n            sns.distplot(data[col],label=col)\n            plt.show()\n    except ValueError:\n        pass\n    \n    \n\nDF=object_data.join(numeric_data_on)\n\nDF.columns\n\ndata=DF.drop([\"dt_of_fr\",\"fr_yn\",\"bldng_us\",\"bldng_archtctr\",\"dt_of_athrztn\",\"bldng_us_clssfctn\",\"jmk\",\"rgnl_ar_nm\",\"rgnl_ar_nm2\",\"lnd_us_sttn_nm\",\"rd_sd_nm\",\"emd_nm\",\"mlt_us_yn\",\\\n             \"trgt_crtr\",\"fr_fghtng_fclt_spcl_css_5_yn\",\"fr_fghtng_fclt_spcl_css_6_yn\",\"us_yn\",\"dngrs_thng_yn\",\"slf_fr_brgd_yn\",\"blk_dngrs_thng_mnfctr_yn\",\"cltrl_hrtg_yn\"],axis=1)\n\ndata.fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_index=data[\"id\"]\n\ndata=data.set_index(\"id\")\n\n# 스케일링 후 다시 칼럼 네임 붙이기\ncolnames=data.columns\n\ntransformer = RobustScaler().fit(data)\ndata=transformer.transform(data)\n\n\ndata=pd.DataFrame(data)\n\ndata.columns=colnames\n\nprint('NON FIRE', round(data['target_value'].value_counts()[0]/len(data) * 100,2), '% of the dataset')\nprint('FIRE', round(data['target_value'].value_counts()[1]/len(data) * 100,2), '% of the dataset')\n\nX = data.drop('target_value', axis=1)\ny = data['target_value']\n\n# Feature Selection\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()\n\nfeat_importances.sort_values(ascending=False)\n\n중요_변수_리스트=feat_importances[feat_importances>0.002].index.tolist()\n\n중요_변수_리스트.append(\"target_value\")\n\ndata[중요_변수_리스트]\n\nprint('NON FIRE', round(data['target_value'].value_counts()[0]/len(data) * 100,2), '% of the dataset')\nprint('FIRE', round(data['target_value'].value_counts()[1]/len(data) * 100,2), '% of the dataset')\n\nX = data.drop('target_value', axis=1)\ny = data['target_value']\n\n# 타겟의 샘플 수\ntarget_sample=data.groupby('target_value').count().iloc[1,1]\nprint(target_sample,\"and\",data.groupby('target_value').count().iloc[0,1])\n\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n\n# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.\n# original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Check the Distribution of the labels\n\n\n# Turn into an array\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)\n\nprint('Label Distributions: \\n')\nprint(train_counts_label/ len(original_ytrain))\nprint(test_counts_label/ len(original_ytest))\n\n# Random Under-Sampling\n\ndata = data.sample(frac=1)\n\n# amount of fraud classes 492 rows.\nReturn = data.loc[data['target_value'] == 1]\nNon_Return = data.loc[data['target_value'] == 0][:target_sample]\n\nnormal_distributed_df = pd.concat([Return, Non_Return])\n\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n\nnew_df.head()\n\nprint('Distribution of the Classes in the subsample dataset')\nprint(new_df['target_value'].value_counts()/len(new_df))\n\ncolors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot('target_value', data=new_df, palette=colors)\nplt.title('Equally Distributed Classes', fontsize=14)\nplt.show()\n\nf, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n\n# Entire DataFrame\ncorr = data.corr()\nsns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\nax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=14)\n\n\nsub_sample_corr = new_df.corr()\nsns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)\nax2.set_title('SubSample Correlation Matrix \\n (use for reference)', fontsize=14)\nplt.show()\n\n# New_df is from the random undersample data (fewer instances)\nX_us = new_df.drop('target_value', axis=1)\ny_us = new_df['target_value']\n\n# This is explicitly used for undersampling.\nX_train, X_test, y_train, y_test = train_test_split(X_us, y_us, test_size=0.2, random_state=42)\n\n# Turn the values into an array for feeding the classification algorithms.\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values\n\n# Let's implement simple classifiers\n\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n}\n\n# Wow our scores are getting even high scores even when applying cross validation.\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 4) * 100, \"% accuracy score\")\n\nbest_xgb_model = xgboost.XGBClassifier(colsample_bytree=0.4,\n                 gamma=0.3,                 \n                 learning_rate=0.1,\n                 max_depth=5,\n                 min_child_weight=1.5,\n                 n_estimators=10000,                                                                    \n                 reg_alpha=1e-05,\n                 reg_lambda=0.01,\n                 subsample=0.95,\n                 )\n\nbest_xgb_model.fit(X_train,y_train)\ntraining_score = cross_val_score(best_xgb_model, X_train, y_train, cv=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_xgb_model.fit(X_train,y_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 가장 우수한 성능을 보이는 SVC\n\nmodel_SVC=SVC()\nmodel_SVC.fit(X=X_train, y=y_train)\n\ny_pred_SVC = model_SVC.predict(X_test)\n\n# 선형 커널 만들어보기\nfrom sklearn.svm import LinearSVC\nmodel_LSVC = LinearSVC()\nmodel_LSVC.fit(X_train,y_train)\n\n# proba = model_LSVC.decision_function(X_train)\n# prob = (proba - proba.min()) / (proba.max() - proba.min())\n# print('Prob[0]: %.3f' % (1-prob[0]))\n# print('Prob[1]: %.3f' % (prob[0]))\n\ncross_val_score(model_LSVC, X_train, y_train, cv=5)\n\n# Logistic Regression model\n\nmodel_LR =LogisticRegression()\nmodel_LR.fit(X_train, y_train)\n\n\ny_pred_LR = model_LR.predict(X_test)\n\n\nmodel_LR.coef_\n\nmodel_RF =RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)\nmodel_RF.fit(X_train, y_train)\n# Random Forest model\n\ny_pred_RF = model_RF.predict(X_test)\n\n# for name, importance in zip(X_train, model_RF.feature_importances_):\n#     print(name, \"=\", importance)\n\nX_train1=pd.DataFrame(X_train)\n\n# RF 모델의 피처 중요도 플롯\n\nfeat_importances = pd.Series(model_RF.feature_importances_, index=X_train1.columns)\nfeat_importances.nlargest(15).plot(kind='barh')\n\n# y_pred = best_xgb_model.predict(X_test)\n\ny_pred2 = best_xgb_model.predict(X.values)\n\ny_pred2 = model_LSVC.predict(X)\n\n#original_Xtest, original_ytest\n\n예측=model_SVC.predict(original_Xtest)\n정답=original_ytest\n\n#original_Xtest, original_ytest\n\n예측=model_SVC.predict(X_test)\n정답=y_test\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(정답, 예측)\nprint(cm)\n\nplt.clf()\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\nclassNames = ['Negative','Positive']\nplt.title('Versicolor or Not Versicolor Confusion Matrix - Test Data')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\nplt.show()\n\nimport sklearn.metrics\n\nprint(sklearn.metrics.recall_score(정답,예측))\nprint(sklearn.metrics.precision_score(정답,예측))\n\nprint(sklearn.metrics.accuracy_score(정답,예측))\nprint(\"이게중요!\",sklearn.metrics.f1_score(정답,예측))\nprint(sklearn.metrics.precision_recall_curve(정답,예측))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import lightgbm as lgb\n\n# create dataset for lightgbm\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n\n# specify your configurations as a dict\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_error',\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'verbose': 1\n}\n\nprint('Starting training...')\n\nparams = {\n        'task': 'train',\n        'objective': 'binary',    # 2値分類の指定\n        'metric': 'binary_error', # 誤答率の割合\n        'verbose': 1\n}\n\n# params = {\n#     'objective' :'binary',\n#     'learning_rate' : 0.02,\n#     'num_leaves' : 76,\n#     'feature_fraction': 0.64, \n#     'bagging_fraction': 0.8, \n#     'bagging_freq':1,\n#     'boosting_type' : 'gbdt',\n#     'metric': 'binary_logloss'\n# }\n\n# gbm = lgb.train(params,\n#             lgb_train,\n#             num_boost_round=50,       \n#             early_stopping_rounds=10,\n#             valid_sets=[lgb_train,lgb_eval],\n#             valid_names=['train', 'eval'])\n\n# train\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=20,\n                valid_sets=lgb_eval,\n                early_stopping_rounds=5)\n\nprint('Saving model...')\n# save model to file\n# gbm.save_model('model.txt')\n\nprint('Starting predicting...')\n# predict\n\n예측 = gbm.predict(original_Xtest, num_iteration=gbm.best_iteration)\n\n예측[예측>=0.5]=1\n예측[예측<0.5]=0\n\n정답=original_ytest","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}