{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## `[Preposition]`\n",
    "\n",
    "#### 1. 10월 한달간 구매하는 고객을 고객의 특성을 활용해 예측할 수 있다\n",
    "\n",
    "​\n",
    "\n",
    "## `[Scenario]`\n",
    "\n",
    "#### 1. 10월 10일까지 구매한 데이터를 바탕으로 학습을 진행\n",
    "\n",
    "#### 2. 10월 10~30일 사이에 구매한 결과를 바탕으로 모델 평가\n",
    "\n",
    "#### 3. 만약 실효성 있는 결과가 나올 경우, 구매 고객을 예측할 때 보조지표로 활용할 수 있다는 것을 의미\n",
    "\n",
    "​\n",
    "\n",
    "## `사용 데이터`\n",
    "\n",
    "​\n",
    "\n",
    "### `1. 1001시점_online_모델용_191001.csv (from Practice.ipynb)`\n",
    "\n",
    "​\n",
    "\n",
    "##### 1) 관수 선임님 폴더에 있는 \"1001시점_online_모델용_191001.csv\",즉 멤버별 온라인 구매 패턴 데이터를 활용\n",
    "\n",
    "​\n",
    "\n",
    "##### 2) 포함 칼럼 :'MBR_NO', 'Y1_총구매일수', 'Y1_총구매건수', 'Y1_총구매금액', '최종구매경과기간(일)', '회원가입경과기간(월)', '구매주기평균', '단기구매주기여부', '3M_WEB_CNT', '2M_WEB_CNT', '1M_WEB_CNT', '3W_WEB_CNT', '2W_WEB_CNT', '1WK_WEB_CNT', 'SMS_RECPTN_AGR_YN'\n",
    "\n",
    "​\n",
    "\n",
    "##### 3) 데이터 사이즈 : 192792, 15\n",
    "\n",
    "​\n",
    "\n",
    "##### 4) 데이터 전처리 : 단기구매주기 여부를 0/1 encoding하고, WEB_CNT(주기별 인터넷 로그 카운트)가 Null인 경우 0으로 fill_na 해줌\n",
    "\n",
    "​\n",
    "\n",
    "====================================================================\n",
    "\n",
    "​\n",
    "\n",
    "### `2. online/ONLINE_ORDER_YEAR.parquet (from Practice.ipynb)`\n",
    "\n",
    "​\n",
    "\n",
    "##### 1) 온라인 구매 내역을 담은 파케이 형식 데이터로, 현재 시점 이전의 주문 데이터를 포함하고 있음\n",
    "\n",
    "​\n",
    "\n",
    "##### 2) 포함 칼럼 : 'MBR_NO', 'ORD_DT', 'ORD_NO', 'GOD_NO', 'SAP_GOD_NO', 'FINAL_ORD_QTY', 'ORD_AMT'\n",
    "\n",
    "​\n",
    "\n",
    "##### 3) 데이터 사이즈 : 1583776, 7\n",
    "\n",
    "​\n",
    "\n",
    "##### 4) 데이터 전처리 : 1번 데이터와 멤버코드를 기준으로 병합하여, 멤버 별 구매 내역을 DataFrame으로 만듦 -> 주문일자 데이터를 수치타입으로 변경 -> target_value 칼럼을 추가해 10월 이후의 주문 내역에 1의 값을 추가함\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "### ` 3. online/DW_MBR (from Online_analysis.ipynb)`\n",
    "\n",
    "​\n",
    "\n",
    "##### 1) 온라인 멤버의 정보를 담은 데이터로, 멤버십 정보와 성별/생년월일 등의 인구통계적 정보를 포함하고 있음. 이 중 수신 동의 고객만 남기고, 중복 고객을 제거하여 40만6천을 남김\n",
    "\n",
    "​\n",
    "\n",
    "##### 2) 포함 칼럼 : 'MBR_NO', 'MBR_TP_CD', 'MBR_ATRB_CD', 'MBR_BRTHDY', 'MBR_SEX_SECT_CD', 'JOIN_DT', 'SMS_RECPTN_AGR_YN', 'CRM_GRD_CD', 'ONLNE_GRD_CD' \n",
    "\n",
    "​\n",
    "\n",
    "##### 3) 데이터 사이즈 : 406242, 9\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "##### 4) 데이터 스키마 : MBR_NO -> ['MB000000000000000'] / MBR_TP_CD -> ['UNITY_MBR' 'ONLINE_MBR'] / MBR_ATRB_CD -> ['GNRL_MBR' 'CHEIL_FSHN' 'CHEIL_FSHN_RTL' 'GRPCO' 'CHEIL'] / MBR_BRTHDY -> '19270801' / MBR_SEX_SECT_CD -> ['FEMALE' 'MALE' None]\n",
    "\n",
    "##### JOIN_DT -> ['2000-00-00T00:00:00.000000000'] / SMS_RECPTN_AGR_YN -> ['Y'] / CRM_GRD_CD -> ['FAM' None 'GLD' 'SLVR' 'DRMC' 'PLTN' 'LTNCY'] / ONLNE_GRD_CD -> ['VIP' None] \n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "#### 5) 데이터 전처리\n",
    "\n",
    "##### 1] MBR_TP_CD : 'UNITY_MBR'은 336426, 'ONLINE_MBR'은 69816으로 상대적 소수인 ONLINE_MBR을 1로 encoding\n",
    "\n",
    "##### 2] MBR_ATRB_CD : 'GNRL_MBR'이 400227, 'GRPCO'은 5051, 나머지는 200대. 인코딩하지 않고 유지함.\n",
    "\n",
    "##### 3] BRTHDY_MNTH_YN : 9/10월에 생일이 들어있는 여부를 encoding함. yes를 1로 encode.\n",
    "\n",
    "##### 4] MBR_AGE : 생년월일로부터 나이를 추출\n",
    "\n",
    "##### 5] MBR_SEX_SECT_CD : 남자는 0, 여자는 1. na값은 대다수인 여성으로 채워줌. \n",
    "\n",
    "##### 6] ONLNE_GRD_CD : VIP를 1, 그렇지 않으면 0으로 encode\n",
    "\n",
    "##### 7] CRM_GRD_CD : CRM 등급은 그냥 둠. (None값 포함되어 있음)\n",
    "\n",
    "##### -> 이 결과를 one_hot 하여 새로운 DF를 만들고, 이를 \"Online_complete.csv\"로 저장\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "​\n",
    "\n",
    "# 개선점\n",
    "\n",
    "​\n",
    "\n",
    "## 연령 코드가 제대로 binning 되어 있지 않음.\n",
    "\n",
    "​\n",
    "\n",
    "## CRM_GRD_CD 등의 범주형 변수는 목표값과의 차이를 살펴 통합하는 방식으로 차원 축소\n",
    "\n",
    "​\n",
    "\n",
    "## 목표값의 재정의가 필요함. -> 전환율 ? 10월 이후 구매를 했는지 여부. 즉, target_value\n",
    "\n",
    "​\n",
    "\n",
    "## 기존 모델에 target_value를 사용해버림. 이는 모델이 cheating할 수 있다는 것을 의미.\n",
    "\n",
    "​\n",
    "\n",
    "## \n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import warnings\n",
    "import itertools as IT\n",
    "import glob\n",
    "import datetime as dt\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.path.insert(0, '/data/aithe/data/customercube/code/common/')\n",
    "# import common_path\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from xgboost import plot_importance\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV   #Perforing grid search\n",
    "\n",
    "#Common Model Algorithms\n",
    "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "import xgboost\n",
    "\n",
    "#Common Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "#Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.style.use('ggplot')\n",
    "sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 12,8\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows',100)\n",
    "pd.set_option('display.max_columns',500)\n",
    "pd.set_option('display.width',500)\n",
    "pd.set_option('max_info_columns',500)\n",
    "pd.set_option('display.float_format', lambda x : '%.3f' % x )\n",
    "\n",
    "day_test_end=\"20191010\"\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "\n",
    "import datetime as dt\n",
    "today = dt.datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "%%time\n",
    "import common_path_MH\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "data=pd.read_csv(\"hypo0_prepro_comp.csv\")\n",
    "\n",
    "data\n",
    "\n",
    "data.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "\n",
    "data_index=data[\"MBR_NO\"]\n",
    "\n",
    "data=data.set_index(\"MBR_NO\")\n",
    "\n",
    "\n",
    "\n",
    "# 스케일링 후 다시 칼럼 네임 붙이기\n",
    "colnames=data.columns\n",
    "\n",
    "transformer = RobustScaler().fit(data)\n",
    "data=transformer.transform(data)\n",
    "\n",
    "\n",
    "data=pd.DataFrame(data)\n",
    "\n",
    "data.columns=colnames\n",
    "\n",
    "# 타겟의 샘플 수\n",
    "target_sample=data.groupby('target_value').count().iloc[1,1]\n",
    "print(target_sample,\"and\",data.groupby('target_value').count().iloc[0,1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('NON RETURN', round(data['target_value'].value_counts()[0]/len(data) * 100,2), '% of the dataset')\n",
    "print('RETURN', round(data['target_value'].value_counts()[1]/len(data) * 100,2), '% of the dataset')\n",
    "\n",
    "X = data.drop('target_value', axis=1)\n",
    "y = data['target_value']\n",
    "\n",
    "sss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n",
    "    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.\n",
    "# original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the Distribution of the labels\n",
    "\n",
    "# Turn into an array\n",
    "original_Xtrain = original_Xtrain.values\n",
    "original_Xtest = original_Xtest.values\n",
    "original_ytrain = original_ytrain.values\n",
    "original_ytest = original_ytest.values\n",
    "\n",
    "# See if both the train and test label distribution are similarly distributed\n",
    "train_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\n",
    "test_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\n",
    "print('-' * 100)\n",
    "\n",
    "print('Label Distributions: \\n')\n",
    "print(train_counts_label/ len(original_ytrain))\n",
    "print(test_counts_label/ len(original_ytest))\n",
    "\n",
    "# Random Under-Sampling\n",
    "\n",
    "new_df.to_csv(\"hypo0_1.csv\",encoding=\"CP949\")\n",
    "\n",
    "data = data.sample(frac=1)\n",
    "\n",
    "# amount of fraud classes 492 rows.\n",
    "Return = data.loc[data['target_value'] == 1]\n",
    "Non_Return = data.loc[data['target_value'] == 0][:target_sample]\n",
    "\n",
    "normal_distributed_df = pd.concat([Return, Non_Return])\n",
    "\n",
    "# Shuffle dataframe rows\n",
    "new_df = normal_distributed_df.sample(frac=1, random_state=42)\n",
    "\n",
    "new_df.head()\n",
    "\n",
    "print('Distribution of the Classes in the subsample dataset')\n",
    "print(new_df['target_value'].value_counts()/len(new_df))\n",
    "\n",
    "colors = [\"#0101DF\", \"#DF0101\"]\n",
    "\n",
    "\n",
    "sns.countplot('target_value', data=new_df, palette=colors)\n",
    "plt.title('Equally Distributed Classes', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n",
    "\n",
    "# Entire DataFrame\n",
    "corr = data.corr()\n",
    "sns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\n",
    "ax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=14)\n",
    "\n",
    "\n",
    "sub_sample_corr = new_df.corr()\n",
    "sns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)\n",
    "ax2.set_title('SubSample Correlation Matrix \\n (use for reference)', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# New_df is from the random undersample data (fewer instances)\n",
    "X_us = new_df.drop('target_value', axis=1)\n",
    "y_us = new_df['target_value']\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import collections\n",
    "# Other Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "#from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "#from imblearn.under_sampling import NearMiss\n",
    "#from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "# Our data is already scaled we should split our training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# This is explicitly used for undersampling.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_us, y_us, test_size=0.2, random_state=42)\n",
    "\n",
    "# Turn the values into an array for feeding the classification algorithms.\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "# Let's implement simple classifiers\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisiticRegression\": LogisticRegression(),\n",
    "    \"KNearest\": KNeighborsClassifier(),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "\n",
    "# Wow our scores are getting even high scores even when applying cross validation.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "for key, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 4) * 100, \"% accuracy score\")\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "\n",
    "# Use GridSearchCV to find the best parameters.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost\n",
    "\n",
    "best_xgb_model = xgboost.XGBClassifier(colsample_bytree=0.4,\n",
    "                 gamma=0.3,                 \n",
    "                 learning_rate=0.1,\n",
    "                 max_depth=5,\n",
    "                 min_child_weight=1.5,\n",
    "                 n_estimators=10000,                                                                    \n",
    "                 reg_alpha=1e-05,\n",
    "                 reg_lambda=0.01,\n",
    "                 subsample=0.95,\n",
    "                 )\n",
    "\n",
    "best_xgb_model.fit(X_train,y_train)\n",
    "\n",
    "training_score = cross_val_score(best_xgb_model, X_train, y_train, cv=5)\n",
    "\n",
    "# 가장 우수한 성능을 보이는 SVC\n",
    "\n",
    "model_SVC=SVC()\n",
    "model_SVC.fit(X=X_train, y=y_train)\n",
    "\n",
    "\n",
    "\n",
    "y_pred_SVC = model_SVC.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 선형 커널 만들어보기\n",
    "from sklearn.svm import LinearSVC\n",
    "model_LSVC = LinearSVC()\n",
    "model_LSVC.fit(X_train,y_train)\n",
    "\n",
    "# proba = model_LSVC.decision_function(X_train)\n",
    "# prob = (proba - proba.min()) / (proba.max() - proba.min())\n",
    "# print('Prob[0]: %.3f' % (1-prob[0]))\n",
    "# print('Prob[1]: %.3f' % (prob[0]))\n",
    "\n",
    "cross_val_score(model_LSVC, X_train, y_train, cv=5)\n",
    "\n",
    "\n",
    "\n",
    "# Logistic Regression model\n",
    "\n",
    "model_LR =LogisticRegression()\n",
    "model_LR.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_LR = model_LR.predict(X_test)\n",
    "\n",
    "\n",
    "model_LR.coef_\n",
    "\n",
    "model_RF =RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)\n",
    "model_RF.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Random Forest model\n",
    "\n",
    "y_pred_RF = model_RF.predict(X_test)\n",
    "\n",
    "\n",
    "for name, importance in zip(X_train, model_RF.feature_importances_):\n",
    "    print(name, \"=\", importance)\n",
    "\n",
    "X_train1=pd.DataFrame(X_train)\n",
    "\n",
    "# RF 모델의 피처 중요도 플롯\n",
    "\n",
    "feat_importances = pd.Series(model_RF.feature_importances_, index=X_train1.columns)\n",
    "feat_importances.nlargest(15).plot(kind='barh')\n",
    "\n",
    "y_pred = best_xgb_model.predict(X_test)\n",
    "\n",
    "\n",
    "X_train\n",
    "\n",
    "y_pred2 = best_xgb_model.predict(X.values)\n",
    "\n",
    "\n",
    "y_pred2 = model_LSVC.predict(X)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y, y_pred2)\n",
    "print(cm)\n",
    "\n",
    "plt.clf()\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "classNames = ['Negative','Positive']\n",
    "plt.title('Versicolor or Not Versicolor Confusion Matrix - Test Data')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "print(sklearn.metrics.recall_score(y,y_pred2))\n",
    "print(sklearn.metrics.precision_score(y,y_pred2))\n",
    "\n",
    "print(sklearn.metrics.accuracy_score(y,y_pred2))\n",
    "print(sklearn.metrics.f1_score(y,y_pred2))\n",
    "print(sklearn.metrics.precision_recall_curve(y,y_pred2))\n",
    "\n",
    "# Feature importance\n",
    "\n",
    "# 피처 중요도를 살펴본 결과, 최종구매경과기간(일) 특성이 가장 중요한 것으로 드러남\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "def f_importances(coef, names):\n",
    "    imp = coef\n",
    "    imp,names = zip(*sorted(zip(imp,names)))\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.show()\n",
    "\n",
    "f_importances(model_LSVC.coef_[0], list(X.columns))\n",
    "\n",
    "\n",
    "# AUROC 구하기\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import interp\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "\n",
    "# Learn to predict each class against the other\n",
    "classifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True,\n",
    "                                 random_state=random_state))\n",
    "\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "\n",
    "\n",
    "\n",
    "y_score2 = model_LSVC.fit(X=X_train, y=y_train).decision_function(X_test)\n",
    "\n",
    "n_classes=len(y_score2)\n",
    "\n",
    "\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test, y_score2)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score2.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "pd.DataFrame(fpr).T\n",
    "\n",
    "fpr[2]\n",
    "\n",
    "tpr[2]\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 10월 11~30일 데이터를 기존 모델에 넣어 맞춰보기 \n",
    "\n",
    "# 10월 11~30일 사이의 구매 고객을 정리해야함\n",
    "# 이를 위해, 데이터의 기간을 설정해 target_value 달아주고\n",
    "# X, Y를 분리시킨 다음\n",
    "# X를 투입해 결과값을 pred에 저장하고,\n",
    "# Y와 대치하여 confusion matrix 만들기\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data2=pd.read_csv(\"hypo0_prepro_comp_1010.csv\")\n",
    "\n",
    "data2\n",
    "\n",
    "data2.drop(\"Unnamed: 0\",axis=1,inplace=True)\n",
    "\n",
    "data2=data2.set_index(\"MBR_NO\")\n",
    "\n",
    "# 스케일링 후 다시 칼럼 네임 붙이기\n",
    "colnames2=data2.columns\n",
    "\n",
    "transformer2 = RobustScaler().fit(data2)\n",
    "data2=transformer2.transform(data2)\n",
    "\n",
    "\n",
    "data2=pd.DataFrame(data2)\n",
    "\n",
    "data2.columns=colnames\n",
    "\n",
    "# 타겟의 샘플 수\n",
    "target_sample2=data2.groupby('target_value').count().iloc[1,1]\n",
    "print(target_sample2,\"and\",data2.groupby('target_value').count().iloc[0,1])\n",
    "\n",
    "\n",
    "\n",
    "print('NON RETURN', round(data2['target_value'].value_counts()[0]/len(data2) * 100,2), '% of the dataset')\n",
    "print('RETURN', round(data2['target_value'].value_counts()[1]/len(data2) * 100,2), '% of the dataset')\n",
    "\n",
    "X2 = data2.drop('target_value', axis=1)\n",
    "y2 = data2['target_value']\n",
    "\n",
    "sss2 = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
    "\n",
    "for train_index, test_index in sss2.split(X2, y2):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    original_Xtrain, original_Xtest = X2.iloc[train_index], X2.iloc[test_index]\n",
    "    original_ytrain, original_ytest = y2.iloc[train_index], y2.iloc[test_index]\n",
    "\n",
    "# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.\n",
    "# original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the Distribution of the labels\n",
    "# Turn into an array\n",
    "original_Xtrain2 = original_Xtrain.values\n",
    "original_Xtest2 = original_Xtest.values\n",
    "original_ytrain2 = original_ytrain.values\n",
    "original_ytest2 = original_ytest.values\n",
    "\n",
    "# See if both the train and test label distribution are similarly distributed\n",
    "train_unique_label, train_counts_label = np.unique(original_ytrain2, return_counts=True)\n",
    "test_unique_label, test_counts_label = np.unique(original_ytest2, return_counts=True)\n",
    "print('-' * 100)\n",
    "\n",
    "print('Label Distributions: \\n')\n",
    "print(train_counts_label/ len(original_ytrain2))\n",
    "print(test_counts_label/ len(original_ytest2))\n",
    "\n",
    "y_pred2 = model_RF.predict(X2)\n",
    "\n",
    "y_pred2 = model_LR.predict(X2)\n",
    "\n",
    "\n",
    "y_pred2 = model_SVC.predict(X2)\n",
    "\n",
    "y_pred2 = model_LSVC.predict(X2)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y2, y_pred2)\n",
    "print(cm)\n",
    "\n",
    "plt.clf()\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "classNames = ['Negative','Positive']\n",
    "plt.title('Versicolor or Not Versicolor Confusion Matrix - Test Data')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "print(sklearn.metrics.recall_score(y2,y_pred2))\n",
    "print(sklearn.metrics.precision_score(y2,y_pred2))\n",
    "\n",
    "print(sklearn.metrics.accuracy_score(y2,y_pred2))\n",
    "print(sklearn.metrics.f1_score(y2,y_pred2))\n",
    "print(sklearn.metrics.precision_recall_curve(y2,y_pred2))\n",
    "\n",
    "\n",
    "\n",
    "# Feature importance\n",
    "\n",
    "# 피처 중요도를 살펴본 결과, 최종구매경과기간(일) 특성이 가장 중요한 것으로 드러남\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "def f_importances(coef, names):\n",
    "    imp = coef\n",
    "    imp,names = zip(*sorted(zip(imp,names)))\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.show()\n",
    "\n",
    "f_importances(model_LSVC.coef_[0], list(X2.columns))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# AUROC 구하기\n",
    "\n",
    "# This is explicitly used for undersampling.\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=42)\n",
    "\n",
    "# Turn the values into an array for feeding the classification algorithms.\n",
    "X_train2 = X_train2.values\n",
    "X_test2 = X_test2.values\n",
    "y_train2 = y_train2.values\n",
    "y_test2 = y_test2.values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "\n",
    "y_score2 = model_LSVC.fit(X=X_train2, y=y_train2).decision_function(X_test2)\n",
    "\n",
    "n_classes=len(y_score2)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test2, y_score2)\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "    \n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test2.ravel(), y_score2.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import imblearn\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "\n",
    "print('Length of X (train): {} | Length of y (train): {}'.format(len(original_Xtrain), len(original_ytrain)))\n",
    "print('Length of X (test): {} | Length of y (test): {}'.format(len(original_Xtest), len(original_ytest)))\n",
    "\n",
    "# List to append the score and then find the average\n",
    "accuracy_lst = []\n",
    "precision_lst = []\n",
    "recall_lst = []\n",
    "f1_lst = []\n",
    "auc_lst = []\n",
    "\n",
    "# Classifier with optimal parameters\n",
    "# log_reg_sm = grid_log_reg.best_estimator_\n",
    "log_reg_sm = LogisticRegression()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n",
    "\n",
    "\n",
    "# Implementing SMOTE Technique \n",
    "# Cross Validating the right way\n",
    "# Parameters\n",
    "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "for train, test in sss.split(original_Xtrain, original_ytrain):\n",
    "    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg) # SMOTE happens during Cross Validation not before..\n",
    "    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n",
    "    best_est = rand_log_reg.best_estimator_\n",
    "    prediction = best_est.predict(original_Xtrain[test])\n",
    "    \n",
    "    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n",
    "    precision_lst.append(precision_score(original_ytrain[test], prediction))\n",
    "    recall_lst.append(recall_score(original_ytrain[test], prediction))\n",
    "    f1_lst.append(f1_score(original_ytrain[test], prediction))\n",
    "    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n",
    "    \n",
    "print('---' * 45)\n",
    "print('')\n",
    "print(\"accuracy: {}\".format(np.mean(accuracy_lst)))\n",
    "print(\"precision: {}\".format(np.mean(precision_lst)))\n",
    "print(\"recall: {}\".format(np.mean(recall_lst)))\n",
    "print(\"f1: {}\".format(np.mean(f1_lst)))\n",
    "print('---' * 45)\n",
    "\n",
    "labels = ['No Fraud', 'Fraud']\n",
    "smote_prediction = best_est.predict(original_Xtest)\n",
    "print(classification_report(original_ytest, smote_prediction, target_names=labels))\n",
    "\n",
    "n_classes=len(y_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
