{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import warnings\n",
    "import itertools as IT\n",
    "import glob\n",
    "import datetime as dt\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# from xgboost import plot_importance\n",
    "from sklearn.model_selection import cross_val_score,KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV   #Perforing grid search\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "# #Common Model Algorithms\n",
    "# from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
    "# import xgboost\n",
    "\n",
    "#Common Model Helpers\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn import feature_selection\n",
    "from sklearn import model_selection\n",
    "from sklearn import metrics\n",
    "\n",
    "#Visualization\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "%matplotlib inline\n",
    "mpl.style.use('ggplot')\n",
    "sns.set_style('white')\n",
    "pylab.rcParams['figure.figsize'] = 12,8\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows',100)\n",
    "pd.set_option('display.max_columns',500)\n",
    "pd.set_option('display.width',500)\n",
    "pd.set_option('max_info_columns',500)\n",
    "pd.set_option('display.float_format', lambda x : '%.3f' % x )\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import collections\n",
    "# Other Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "#from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "#from imblearn.under_sampling import NearMiss\n",
    "#from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import collections\n",
    "# Other Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "# Our data is already scaled we should split our training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "# Use GridSearchCV to find the best parameters.\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "data=pd.read_csv(\"PJT002_train.csv\",encoding=\"UTF-8-sig\")\n",
    "\n",
    "data.shape\n",
    "\n",
    "print('NON FIRE', round(data['fr_yn'].value_counts()[0]/len(data) * 100,2), '% of the dataset')\n",
    "print('FIRE', round(data['fr_yn'].value_counts()[1]/len(data) * 100,2), '% of the dataset')\n",
    "\n",
    "data.isnull().sum()\n",
    "\n",
    "# 오브젝트 먼저 전처리\n",
    "\n",
    "data.iloc[:,2].dtype=='O'\n",
    "\n",
    "object_list=[]\n",
    "for col in data:\n",
    "    if data[col].dtype ==\"object\":\n",
    "        object_list.append(col)\n",
    "\n",
    "# 수치형 데이터 따로 정리\n",
    "\n",
    "object_data=data[object_list[:]]\n",
    "\n",
    "numeric_list=list(set(data.columns.tolist()) - set(object_list))\n",
    "numeric_list=sorted(numeric_list)\n",
    "numeric_data=data[numeric_list[:]]\n",
    "\n",
    "\n",
    "# 화재 발생 시기 데이터 타입 변경\n",
    "object_data['dt_of_fr']=object_data['dt_of_fr'].astype('datetime64[ns]')\n",
    "\n",
    "### bldng_us(사용용도)\n",
    "\n",
    "object_data[\"주거용YN\"]=object_data.lnd_us_sttn_nm.apply(lambda x: 1 if x ==\"단독\" else x)\n",
    "object_data[\"주거용YN\"]=object_data.주거용YN.apply(lambda x: 1 if x ==\"연립\" else x)\n",
    "object_data[\"주거용YN\"]=object_data.주거용YN.apply(lambda x: 1 if x ==\"아파트\" else x)\n",
    "object_data[\"주거용YN\"]=object_data.주거용YN.apply(lambda x: 1 if x ==\"다세대\" else x)\n",
    "object_data[\"주거용YN\"]=object_data.주거용YN.apply(lambda x: 1 if x ==\"주거기타\" else x)\n",
    "object_data['주거용YN']=object_data.주거용YN.apply(lambda x: 1 if x==1 else 0)\n",
    "\n",
    "\n",
    "object_data[\"공업용YN\"]=object_data.lnd_us_sttn_nm.apply(lambda x: 1 if x ==\"공업용\" else x)\n",
    "object_data[\"공업용YN\"]=object_data.공업용YN.apply(lambda x: 1 if x ==\"공업기타\" else x)\n",
    "object_data[\"공업용YN\"]=object_data.공업용YN.apply(lambda x: 1 if x ==\"발전소\" else x)\n",
    "object_data[\"공업용YN\"]=object_data.공업용YN.apply(lambda x: 1 if x ==\"유해.혐오시설\" else x)\n",
    "object_data[\"공업용YN\"]=object_data.공업용YN.apply(lambda x: 1 if x ==\"공업나지\" else x)\n",
    "object_data['공업용YN']=object_data.공업용YN.apply(lambda x: 1 if x==1 else 0)\n",
    "\n",
    "\n",
    "object_data[\"농업용YN\"]=object_data.lnd_us_sttn_nm.apply(lambda x: 1 if x ==\"자연림\" else x)\n",
    "object_data[\"농업용YN\"]=object_data.농업용YN.apply(lambda x: 1 if x ==\"답\" else x)\n",
    "object_data[\"농업용YN\"]=object_data.농업용YN.apply(lambda x: 1 if x ==\"전\" else x)\n",
    "object_data[\"농업용YN\"]=object_data.농업용YN.apply(lambda x: 1 if x ==\"조림\" else x)\n",
    "object_data[\"농업용YN\"]=object_data.농업용YN.apply(lambda x: 1 if x ==\"답기타\" else x)\n",
    "object_data[\"농업용YN\"]=object_data.농업용YN.apply(lambda x: 1 if x ==\"과수원\" else x)\n",
    "object_data[\"농업용YN\"]=object_data.농업용YN.apply(lambda x: 1 if x ==\"토지임야\" else x)\n",
    "object_data[\"농업용YN\"]=object_data.농업용YN.apply(lambda x: 1 if x ==\"임야기타\" else x)\n",
    "object_data[\"농업용YN\"]=object_data.농업용YN.apply(lambda x: 1 if x ==\"목장용지\" else x)\n",
    "object_data['농업용YN']=object_data.농업용YN.apply(lambda x: 1 if x==1 else 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# object_data.fr_yn=object_data.fr_yn.apply(lambda x: 1 if x==\"Y\" else 0)\n",
    "\n",
    "# 사용 용도에 따라 주거시설/편의시설 여부로 더미화\n",
    "\n",
    "object_data.bldng_us.unique()\n",
    "\n",
    "object_data['주거시설YN']=object_data.bldng_us.apply(lambda x: 1 if x==\"단독주택\" else x)\n",
    "object_data['주거시설YN']=object_data.주거시설YN.apply(lambda x: 1 if x==\"공동주택\" else x)\n",
    "object_data['주거시설YN']=object_data.주거시설YN.apply(lambda x: 1 if x==1 else 0)\n",
    "\n",
    "object_data['편의시설YN']=object_data.bldng_us.apply(lambda x: 1 if x==\"제1종근린생활시설\" else x)\n",
    "object_data['편의시설YN']=object_data.편의시설YN.apply(lambda x: 1 if x==\"제2종근린생활시설\" else x)\n",
    "object_data['편의시설YN']=object_data.편의시설YN.apply(lambda x: 1 if x==\"문화및집회시설\" else x)\n",
    "object_data['편의시설YN']=object_data.편의시설YN.apply(lambda x: 1 if x==1 else 0)\n",
    "\n",
    "object_data.groupby(\"bldng_us\").size().sort_values(ascending=False)\n",
    "\n",
    "# 6자리 혹은 nan 혹은 10자리임을 알 수 있음.\n",
    "#  sns.distplot(object_data.dt_of_athrztn.astype(str).apply(len))\n",
    "\n",
    "### dt_of_athrztn(건물승인일자)\n",
    "\n",
    "# 건축 날짜의 데이터 타입이 2종류, 따라서 이 모양을 연도 형식으로 바꾸어줄 필요가 있음.\n",
    "\n",
    "object_data.dt_of_athrztn=object_data.dt_of_athrztn.apply(lambda x: str(x)[0:4] if len(str(x))==10 else x)\n",
    "object_data.dt_of_athrztn=object_data.dt_of_athrztn.apply(lambda x: str(x)[0:4] if len(str(x))==8 else x)\n",
    "object_data.dt_of_athrztn=object_data.dt_of_athrztn.apply(lambda x: str(x)[0:4] if len(str(x))==6 else x)\n",
    "\n",
    "dirty_date_idx=object_data.dt_of_athrztn[object_data.dt_of_athrztn.astype(str).apply(len)>4].index\n",
    "\n",
    "object_data.dt_of_athrztn[dirty_date_idx[0]]=\"1980\"\n",
    "object_data.dt_of_athrztn[dirty_date_idx[1]]=\"1890\"\n",
    "object_data.dt_of_athrztn[dirty_date_idx[2]]=\"1890\"\n",
    "object_data.dt_of_athrztn[dirty_date_idx[3]]=\"1990\"\n",
    "object_data.dt_of_athrztn[dirty_date_idx[4]]=\"1990\"\n",
    "object_data.dt_of_athrztn[dirty_date_idx[5]]=\"1982\"\n",
    "object_data.dt_of_athrztn[dirty_date_idx[6]]=\"1978\"\n",
    "object_data.dt_of_athrztn[dirty_date_idx[7]]=\"1994\"\n",
    "\n",
    "#결측치가 아닌 연도 데이터를 시각화 하는 코드\n",
    "\n",
    "#sns.distplot(object_data.dt_of_athrztn[~(object_data.dt_of_athrztn.isna)()].astype(int))\n",
    "\n",
    "말도안돼_연도=object_data.dt_of_athrztn[~(object_data.dt_of_athrztn.isna)()].astype(int)[object_data.dt_of_athrztn[~(object_data.dt_of_athrztn.isna)()].astype(int)>2019].index\n",
    "\n",
    "object_data.dt_of_athrztn[말도안돼_연도]=\"1997\"\n",
    "\n",
    "있는_연도=object_data.dt_of_athrztn[~(object_data.dt_of_athrztn.isna())].astype(int)\n",
    "\n",
    "object_data.dt_of_athrztn.fillna(\"1988\",inplace=True)\n",
    "\n",
    "object_data.dt_of_athrztn=object_data.dt_of_athrztn.astype(int)\n",
    "\n",
    "object_data[\"건물연령\"]=object_data.dt_of_athrztn.apply(lambda x: 2019-x)\n",
    "\n",
    "### rd_sd_nm(도로측면명)\n",
    "\n",
    "object_data.rd_sd_nm.unique()\n",
    "\n",
    "data.isnull().sum(axis=1)[data.isnull().sum(axis=1)<150].index\n",
    "\n",
    "object_data['차량통행YN']=object_data.rd_sd_nm.apply(lambda x: 1 if x==\"세로한면(가)\" else x)\n",
    "object_data['차량통행YN']=object_data.차량통행YN.apply(lambda x: 1 if x==\"세로각지(가)\" else x)\n",
    "object_data['차량통행YN']=object_data.차량통행YN.apply(lambda x: 1 if x==\"소로각지\" else x)\n",
    "object_data['차량통행YN']=object_data.차량통행YN.apply(lambda x: 1 if x==\"중로한면\" else x)\n",
    "object_data['차량통행YN']=object_data.차량통행YN.apply(lambda x: 1 if x==\"중로각지\" else x)\n",
    "object_data['차량통행YN']=object_data.차량통행YN.apply(lambda x: 1 if x==\"소로한면\" else x)\n",
    "object_data['차량통행YN']=object_data.차량통행YN.apply(lambda x: 1 if x==1 else 0)\n",
    "\n",
    "object_data['대로인접YN']=object_data.rd_sd_nm.apply(lambda x: 1 if x==\"중로각지\" else x)\n",
    "object_data['대로인접YN']=object_data.대로인접YN.apply(lambda x: 1 if x==\"중로한면\" else x)\n",
    "object_data['대로인접YN']=object_data.대로인접YN.apply(lambda x: 1 if x==\"광대세각\" else x)\n",
    "object_data['대로인접YN']=object_data.대로인접YN.apply(lambda x: 1 if x==\"광대소각\" else x)\n",
    "object_data['대로인접YN']=object_data.대로인접YN.apply(lambda x: 1 if x==\"광대로한면\" else x)\n",
    "object_data['대로인접YN']=object_data.대로인접YN.apply(lambda x: 1 if x==1 else 0)\n",
    "\n",
    "### bldng_archtctr(건물구조)\n",
    "\n",
    "object_data['목재구조YN']=object_data.bldng_archtctr.apply(lambda x: 1 if x==\"일반목구조\" else x)\n",
    "object_data['목재구조YN']=object_data.목재구조YN.apply(lambda x: 1 if x==\"통나무구조\" else x)\n",
    "object_data['목재구조YN']=object_data.목재구조YN.apply(lambda x: 1 if x==\"목구조\" else x)\n",
    "object_data['목재구조YN']=object_data.목재구조YN.apply(lambda x: 1 if x==1 else 0)\n",
    "\n",
    "### bldng_us_clssfctn(건물용도분류명)\n",
    "\n",
    "### rgnl_ar_nm(용도지역지구명)\n",
    "\n",
    "object_data.rgnl_ar_nm.unique()\n",
    "\n",
    "object_data.groupby(\"rgnl_ar_nm\").size().sort_values(ascending=False)\n",
    "\n",
    "object_data.bldng_us_clssfctn.unique()\n",
    "\n",
    "object_data.groupby(\"bldng_us_clssfctn\").size().sort_values(ascending=False)\n",
    "\n",
    "object_data['주거용YN']=object_data.bldng_us_clssfctn.apply(lambda x: 1 if x==\"주거용\" else 0)\n",
    "\n",
    "object_data['target_value']=object_data.fr_yn.apply(lambda x: 1 if x==\"Y\" else 0)\n",
    "\n",
    "\n",
    "\n",
    "### emd_nm(행정구역명) 데이터\n",
    "\n",
    "sns.distplot(object_data.emd_nm.astype(str).apply(len))\n",
    "\n",
    "object_data.emd_nm[object_data.emd_nm.astype(str).apply(len)[object_data.emd_nm.astype(str).apply(len)<4].index]\n",
    "\n",
    "\n",
    "object_data.emd_nm=object_data.emd_nm.str[4:]\n",
    "\n",
    "object_data.emd_nm[object_data.emd_nm.notnull()]\n",
    "\n",
    "object_data.emd_nm[object_data.emd_nm.isnull()]\n",
    "\n",
    "object_data[\"시YN\"]=object_data.emd_nm[object_data.emd_nm.notnull()].apply(lambda x: \"1\" if x[3]==\"시\" else \"0\")\n",
    "\n",
    "\n",
    "\n",
    "object_data.groupby(\"emd_nm\").size().sort_values(ascending=False).sample(100)\n",
    "\n",
    "# 수치형 데이터\n",
    "\n",
    "numeric_list_not_engry=[s for s in numeric_list if not \"engry\" in s]\n",
    "\n",
    "\n",
    "numeric_data_on=numeric_data.loc[:,numeric_list_not_engry]\n",
    "\n",
    "numeric_data_on.describe()\n",
    "\n",
    "for col in numeric_data_on:\n",
    "    try:\n",
    "        if data[col].dtype !='object':\n",
    "            sns.distplot(data[col],label=col)\n",
    "            plt.show()\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\n",
    "DF=object_data.join(numeric_data_on)\n",
    "\n",
    "DF.columns\n",
    "\n",
    "data=DF.drop([\"dt_of_fr\",\"fr_yn\",\"bldng_us\",\"bldng_archtctr\",\"dt_of_athrztn\",\"bldng_us_clssfctn\",\"jmk\",\"rgnl_ar_nm\",\"rgnl_ar_nm2\",\"lnd_us_sttn_nm\",\"rd_sd_nm\",\"emd_nm\",\"mlt_us_yn\",\\\n",
    "             \"trgt_crtr\",\"fr_fghtng_fclt_spcl_css_5_yn\",\"fr_fghtng_fclt_spcl_css_6_yn\",\"us_yn\",\"dngrs_thng_yn\",\"slf_fr_brgd_yn\",\"blk_dngrs_thng_mnfctr_yn\",\"cltrl_hrtg_yn\"],axis=1)\n",
    "\n",
    "data.fillna(0,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "data_index=data[\"id\"]\n",
    "\n",
    "data=data.set_index(\"id\")\n",
    "\n",
    "# 스케일링 후 다시 칼럼 네임 붙이기\n",
    "colnames=data.columns\n",
    "\n",
    "transformer = RobustScaler().fit(data)\n",
    "data=transformer.transform(data)\n",
    "\n",
    "\n",
    "data=pd.DataFrame(data)\n",
    "\n",
    "data.columns=colnames\n",
    "\n",
    "print('NON FIRE', round(data['target_value'].value_counts()[0]/len(data) * 100,2), '% of the dataset')\n",
    "print('FIRE', round(data['target_value'].value_counts()[1]/len(data) * 100,2), '% of the dataset')\n",
    "\n",
    "X = data.drop('target_value', axis=1)\n",
    "y = data['target_value']\n",
    "\n",
    "# Feature Selection\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,y)\n",
    "print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n",
    "#plot graph of feature importances for better visualization\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(10).plot(kind='barh')\n",
    "plt.show()\n",
    "\n",
    "feat_importances.sort_values(ascending=False)\n",
    "\n",
    "중요_변수_리스트=feat_importances[feat_importances>0.002].index.tolist()\n",
    "\n",
    "중요_변수_리스트.append(\"target_value\")\n",
    "\n",
    "data[중요_변수_리스트]\n",
    "\n",
    "print('NON FIRE', round(data['target_value'].value_counts()[0]/len(data) * 100,2), '% of the dataset')\n",
    "print('FIRE', round(data['target_value'].value_counts()[1]/len(data) * 100,2), '% of the dataset')\n",
    "\n",
    "X = data.drop('target_value', axis=1)\n",
    "y = data['target_value']\n",
    "\n",
    "# 타겟의 샘플 수\n",
    "target_sample=data.groupby('target_value').count().iloc[1,1]\n",
    "print(target_sample,\"and\",data.groupby('target_value').count().iloc[0,1])\n",
    "\n",
    "sss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
    "\n",
    "for train_index, test_index in sss.split(X, y):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n",
    "    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.\n",
    "# original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check the Distribution of the labels\n",
    "\n",
    "\n",
    "# Turn into an array\n",
    "original_Xtrain = original_Xtrain.values\n",
    "original_Xtest = original_Xtest.values\n",
    "original_ytrain = original_ytrain.values\n",
    "original_ytest = original_ytest.values\n",
    "\n",
    "# See if both the train and test label distribution are similarly distributed\n",
    "train_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\n",
    "test_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\n",
    "print('-' * 100)\n",
    "\n",
    "print('Label Distributions: \\n')\n",
    "print(train_counts_label/ len(original_ytrain))\n",
    "print(test_counts_label/ len(original_ytest))\n",
    "\n",
    "# Random Under-Sampling\n",
    "\n",
    "data = data.sample(frac=1)\n",
    "\n",
    "# amount of fraud classes 492 rows.\n",
    "Return = data.loc[data['target_value'] == 1]\n",
    "Non_Return = data.loc[data['target_value'] == 0][:target_sample]\n",
    "\n",
    "normal_distributed_df = pd.concat([Return, Non_Return])\n",
    "\n",
    "# Shuffle dataframe rows\n",
    "new_df = normal_distributed_df.sample(frac=1, random_state=42)\n",
    "\n",
    "new_df.head()\n",
    "\n",
    "print('Distribution of the Classes in the subsample dataset')\n",
    "print(new_df['target_value'].value_counts()/len(new_df))\n",
    "\n",
    "colors = [\"#0101DF\", \"#DF0101\"]\n",
    "\n",
    "sns.countplot('target_value', data=new_df, palette=colors)\n",
    "plt.title('Equally Distributed Classes', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n",
    "\n",
    "# Entire DataFrame\n",
    "corr = data.corr()\n",
    "sns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\n",
    "ax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=14)\n",
    "\n",
    "\n",
    "sub_sample_corr = new_df.corr()\n",
    "sns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)\n",
    "ax2.set_title('SubSample Correlation Matrix \\n (use for reference)', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# New_df is from the random undersample data (fewer instances)\n",
    "X_us = new_df.drop('target_value', axis=1)\n",
    "y_us = new_df['target_value']\n",
    "\n",
    "# This is explicitly used for undersampling.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_us, y_us, test_size=0.2, random_state=42)\n",
    "\n",
    "# Turn the values into an array for feeding the classification algorithms.\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "# Let's implement simple classifiers\n",
    "\n",
    "classifiers = {\n",
    "    \"LogisiticRegression\": LogisticRegression(),\n",
    "    \"KNearest\": KNeighborsClassifier(),\n",
    "    \"Support Vector Classifier\": SVC(),\n",
    "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "# Wow our scores are getting even high scores even when applying cross validation.\n",
    "\n",
    "for key, classifier in classifiers.items():\n",
    "    classifier.fit(X_train, y_train)\n",
    "    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n",
    "    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 4) * 100, \"% accuracy score\")\n",
    "\n",
    "best_xgb_model = xgboost.XGBClassifier(colsample_bytree=0.4,\n",
    "                 gamma=0.3,                 \n",
    "                 learning_rate=0.1,\n",
    "                 max_depth=5,\n",
    "                 min_child_weight=1.5,\n",
    "                 n_estimators=10000,                                                                    \n",
    "                 reg_alpha=1e-05,\n",
    "                 reg_lambda=0.01,\n",
    "                 subsample=0.95,\n",
    "                 )\n",
    "\n",
    "best_xgb_model.fit(X_train,y_train)\n",
    "training_score = cross_val_score(best_xgb_model, X_train, y_train, cv=5)\n",
    "\n",
    "# 가장 우수한 성능을 보이는 SVC\n",
    "\n",
    "model_SVC=SVC()\n",
    "model_SVC.fit(X=X_train, y=y_train)\n",
    "\n",
    "y_pred_SVC = model_SVC.predict(X_test)\n",
    "\n",
    "# 선형 커널 만들어보기\n",
    "from sklearn.svm import LinearSVC\n",
    "model_LSVC = LinearSVC()\n",
    "model_LSVC.fit(X_train,y_train)\n",
    "\n",
    "# proba = model_LSVC.decision_function(X_train)\n",
    "# prob = (proba - proba.min()) / (proba.max() - proba.min())\n",
    "# print('Prob[0]: %.3f' % (1-prob[0]))\n",
    "# print('Prob[1]: %.3f' % (prob[0]))\n",
    "\n",
    "cross_val_score(model_LSVC, X_train, y_train, cv=5)\n",
    "\n",
    "# Logistic Regression model\n",
    "\n",
    "model_LR =LogisticRegression()\n",
    "model_LR.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_LR = model_LR.predict(X_test)\n",
    "\n",
    "\n",
    "model_LR.coef_\n",
    "\n",
    "model_RF =RandomForestClassifier(n_estimators=500, n_jobs=-1, random_state=42)\n",
    "model_RF.fit(X_train, y_train)\n",
    "# Random Forest model\n",
    "\n",
    "y_pred_RF = model_RF.predict(X_test)\n",
    "\n",
    "# for name, importance in zip(X_train, model_RF.feature_importances_):\n",
    "#     print(name, \"=\", importance)\n",
    "\n",
    "X_train1=pd.DataFrame(X_train)\n",
    "\n",
    "# RF 모델의 피처 중요도 플롯\n",
    "\n",
    "feat_importances = pd.Series(model_RF.feature_importances_, index=X_train1.columns)\n",
    "feat_importances.nlargest(15).plot(kind='barh')\n",
    "\n",
    "# y_pred = best_xgb_model.predict(X_test)\n",
    "\n",
    "y_pred2 = best_xgb_model.predict(X.values)\n",
    "\n",
    "y_pred2 = model_LSVC.predict(X)\n",
    "\n",
    "#original_Xtest, original_ytest\n",
    "\n",
    "예측=model_SVC.predict(original_Xtest)\n",
    "정답=original_ytest\n",
    "\n",
    "#original_Xtest, original_ytest\n",
    "\n",
    "예측=model_SVC.predict(X_test)\n",
    "정답=y_test\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(정답, 예측)\n",
    "print(cm)\n",
    "\n",
    "plt.clf()\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Wistia)\n",
    "classNames = ['Negative','Positive']\n",
    "plt.title('Versicolor or Not Versicolor Confusion Matrix - Test Data')\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "tick_marks = np.arange(len(classNames))\n",
    "plt.xticks(tick_marks, classNames, rotation=45)\n",
    "plt.yticks(tick_marks, classNames)\n",
    "s = [['TN','FP'], ['FN', 'TP']]\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]))\n",
    "plt.show()\n",
    "\n",
    "import sklearn.metrics\n",
    "\n",
    "print(sklearn.metrics.recall_score(정답,예측))\n",
    "print(sklearn.metrics.precision_score(정답,예측))\n",
    "\n",
    "print(sklearn.metrics.accuracy_score(정답,예측))\n",
    "print(\"이게중요!\",sklearn.metrics.f1_score(정답,예측))\n",
    "print(sklearn.metrics.precision_recall_curve(정답,예측))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "def f_importances(coef, names):\n",
    "    imp = coef\n",
    "    imp,names = zip(*sorted(zip(imp,names)))\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.show()\n",
    "\n",
    "f_importances(model_SVC.coef_[0], list(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "\n",
    "print('Length of X (train): {} | Length of y (train): {}'.format(len(original_Xtrain), len(original_ytrain)))\n",
    "print('Length of X (test): {} | Length of y (test): {}'.format(len(original_Xtest), len(original_ytest)))\n",
    "\n",
    "# List to append the score and then find the average\n",
    "accuracy_lst = []\n",
    "precision_lst = []\n",
    "recall_lst = []\n",
    "f1_lst = []\n",
    "auc_lst = []\n",
    "\n",
    "# Classifier with optimal parameters\n",
    "# log_reg_sm = grid_log_reg.best_estimator_\n",
    "log_reg_sm = LogisticRegression()\n",
    "\n",
    "rand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing SMOTE Technique \n",
    "# Cross Validating the right way\n",
    "# Parameters\n",
    "log_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "for train, test in sss.split(original_Xtrain, original_ytrain):\n",
    "    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg) # SMOTE happens during Cross Validation not before..\n",
    "    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n",
    "    best_est = rand_log_reg.best_estimator_\n",
    "    prediction = best_est.predict(original_Xtrain[test])\n",
    "    \n",
    "    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n",
    "    precision_lst.append(precision_score(original_ytrain[test], prediction))\n",
    "    recall_lst.append(recall_score(original_ytrain[test], prediction))\n",
    "    f1_lst.append(f1_score(original_ytrain[test], prediction))\n",
    "    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n",
    "    \n",
    "print('---' * 45)\n",
    "print('')\n",
    "print(\"accuracy: {}\".format(np.mean(accuracy_lst)))\n",
    "print(\"precision: {}\".format(np.mean(precision_lst)))\n",
    "print(\"recall: {}\".format(np.mean(recall_lst)))\n",
    "print(\"f1: {}\".format(np.mean(f1_lst)))\n",
    "print('---' * 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['No Fraud', 'Fraud']\n",
    "smote_prediction = best_est.predict(original_Xtest)\n",
    "print(classification_report(original_ytest, smote_prediction, target_names=labels))\n",
    "\n",
    "n_classes=len(y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light GBM 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn import svm\n",
    "\n",
    "def f_importances(coef, names):\n",
    "    imp = coef\n",
    "    imp,names = zip(*sorted(zip(imp,names)))\n",
    "    plt.barh(range(len(names)), imp, align='center')\n",
    "    plt.yticks(range(len(names)), names)\n",
    "    plt.show()\n",
    "\n",
    "f_importances(model_SVC.coef_[0], list(X.columns))\n",
    "import imblearn\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "\n",
    "\n",
    "print('Length of X (train): {} | Length of y (train): {}'.format(len(original_Xtrain), len(original_ytrain)))\n",
    "print('Length of X (test): {} | Length of y (test): {}'.format(len(original_Xtest), len(original_ytest)))\n",
    "\n",
    "# List to append the score and then find the average\n",
    "accuracy_lst = []\n",
    "precision_lst = []\n",
    "recall_lst = []\n",
    "f1_lst = []\n",
    "auc_lst = []\n",
    "\n",
    "# Classifier with optimal parameters\n",
    "# log_reg_sm = grid_log_reg.best_estimator_\n",
    "log_reg_sm = LogisticRegression()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv(\"PJT002_test.csv\")\n",
    "\n",
    "\n",
    "\n",
    "object_list=[]\n",
    "for col in test:\n",
    "    if test[col].dtype ==\"object\":\n",
    "        object_list.append(col)\n",
    "\n",
    "object_list=[]\n",
    "for i in range(len(data.columns.tolist())):\n",
    "    if data[data.columns.tolist()[i]].dtype == 'O':\n",
    "        object_list.append(data.columns.tolist()[i])\n",
    "\n",
    "\n",
    "object_data=test[object_list[:]]\n",
    "\n",
    "numeric_list=list(set(test.columns.tolist()) - set(object_list))\n",
    "numeric_list=sorted(numeric_list)\n",
    "numeric_data=test[numeric_list[:]]\n",
    "\n",
    "\n",
    "\n",
    "object_data['dt_of_fr']=object_data['dt_of_fr'].astype('datetime64[ns]')\n",
    "\n",
    "# object_data.fr_yn=object_data.fr_yn.apply(lambda x: 1 if x==\"Y\" else 0)\n",
    "\n",
    "\n",
    "object_data['주거시설YN']=object_data.bldng_us.apply(lambda x: 1 if x==\"단독주택\" else x)\n",
    "object_data['주거시설YN']=object_data.주거시설YN.apply(lambda x: 1 if x==\"공동주택\" else x)\n",
    "object_data['주거시설YN']=object_data.주거시설YN.apply(lambda x: 1 if x==1 else 0)\n",
    "\n",
    "object_data['편의시설YN']=object_data.bldng_us.apply(lambda x: 1 if x==\"제1종근린생활시설\" else x)\n",
    "\n",
    "object_data['편의시설YN']=object_data.편의시설YN.apply(lambda x: 1 if x==\"제2종근린생활시설\" else x)\n",
    "\n",
    "object_data['편의시설YN']=object_data.편의시설YN.apply(lambda x: 1 if x==\"문화및집회시설\" else x)\n",
    "\n",
    "object_data['편의시설YN']=object_data.편의시설YN.apply(lambda x: 1 if x==1 else 0)\n",
    "​\n",
    "\n",
    "numeric_data_on=numeric_data.iloc[:,[0,1,2,3,4,5,6,68,69,70,71,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
